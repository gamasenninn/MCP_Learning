#!/usr/bin/env python3
"""
ç°¡ç•¥åŒ–ã•ã‚ŒãŸLLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚¹ã‚­ãƒ¼ãƒã‚’æ´»ç”¨ã—ãŸè‡ªç„¶è¨€èªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import asyncio
import json
import os
from typing import Dict, List, Any, Optional
from fastmcp import Client
from openai import AsyncOpenAI
from dotenv import load_dotenv

load_dotenv()

class SimpleLLMClient:
    """LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
    
    def __init__(self, config_file: str = "mcp_servers.json"):
        self.servers = {}
        self.clients = {}
        self.tools_schema = {}
        self.load_config(config_file)
        self.llm = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def load_config(self, config_file: str):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        for server_info in config.get("servers", []):
            self.servers[server_info["name"]] = server_info
    
    async def collect_all_tools(self):
        """å…¨ã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’åé›†"""
        print("ğŸ” ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’åé›†ä¸­...")
        
        for server_name, server_info in self.servers.items():
            if server_name not in self.clients:
                # ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶š
                client = Client(server_info["path"])
                await client.__aenter__()
                await client.ping()
                self.clients[server_name] = client
            
            # ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
            tools = await self.clients[server_name].list_tools()
            self.tools_schema[server_name] = []
            
            for tool in tools:
                tool_info = {
                    "name": tool.name,
                    "description": tool.description or "",
                    "parameters": tool.input_schema if hasattr(tool, 'input_schema') else {}
                }
                self.tools_schema[server_name].append(tool_info)
            
            print(f"  âœ… {server_name}: {len(tools)}å€‹ã®ãƒ„ãƒ¼ãƒ«")
    
    def prepare_tools_for_llm(self) -> str:
        """ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’LLMç”¨ã«æ•´å½¢"""
        tools_description = []
        
        for server_name, tools in self.tools_schema.items():
            for tool in tools:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜ã‚’ç”Ÿæˆ
                params = tool.get('parameters', {})
                params_desc = ""
                if params and 'properties' in params:
                    param_list = []
                    for key, value in params['properties'].items():
                        param_type = value.get('type', 'any')
                        required = key in params.get('required', [])
                        req_mark = " (å¿…é ˆ)" if required else ""
                        param_list.append(f"    - {key}: {param_type}{req_mark}")
                    if param_list:
                        params_desc = "\n  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\n" + "\n".join(param_list)
                
                tool_desc = f"""
{server_name}.{tool['name']}:
  èª¬æ˜: {tool['description']}{params_desc}
"""
                tools_description.append(tool_desc)
        
        return "\n".join(tools_description)
    
    async def select_tool_with_llm(self, query: str) -> Optional[Dict]:
        """LLMã‚’ä½¿ã£ã¦ãƒ„ãƒ¼ãƒ«ã¨å¼•æ•°ã‚’é¸æŠ"""
        tools_desc = self.prepare_tools_for_llm()
        
        prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: {query}

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
{tools_desc}

æœ€é©ãªãƒ„ãƒ¼ãƒ«ã‚’é¸ã³ã€å¿…è¦ãªå¼•æ•°ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚

é‡è¦ãªæ³¨æ„äº‹é …ï¼š
- serverã«ã¯ã€Œcalculatorã€ã€Œdatabaseã€ã€Œweatherã€ã€Œuniversalã€ã®ã„ãšã‚Œã‹ã‚’å…¥ã‚Œã¦ãã ã•ã„
- toolã«ã¯ãƒ„ãƒ¼ãƒ«åã®ã¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ï¼ˆã‚µãƒ¼ãƒãƒ¼åã¯å«ã‚ãªã„ï¼‰
- ä¾‹: server: "calculator", tool: "add" ï¼ˆcalculator.addã§ã¯ãªã„ï¼‰

ä»¥ä¸‹ã®JSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„ï¼š

{{
  "server": "ã‚µãƒ¼ãƒãƒ¼å",
  "tool": "ãƒ„ãƒ¼ãƒ«åã®ã¿",
  "arguments": {{å¼•æ•°ã®ã‚­ãƒ¼ã¨å€¤}},
  "reasoning": "ãªãœã“ã®ãƒ„ãƒ¼ãƒ«ã‚’é¸ã‚“ã ã‹"
}}

JSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = await self.llm.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that selects appropriate tools. Always respond with valid JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0
            )
            
            # JSONå½¢å¼ã§è¿”ã£ã¦ããŸå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
            content = response.choices[0].message.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            
            result = json.loads(content.strip())
            return result
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def execute_tool(self, server_name: str, tool_name: str, arguments: dict):
        """MCPãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
        if server_name not in self.clients:
            print(f"âŒ ã‚µãƒ¼ãƒãƒ¼ {server_name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print(f"   åˆ©ç”¨å¯èƒ½ãªã‚µãƒ¼ãƒãƒ¼: {list(self.clients.keys())}")
            return None
        
        try:
            client = self.clients[server_name]
            print(f"   [ãƒ‡ãƒãƒƒã‚°] call_tool({tool_name}, {arguments})")
            result = await client.call_tool(tool_name, arguments)
            
            # çµæœã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            if hasattr(result, 'content'):
                if isinstance(result.content, list) and result.content:
                    first = result.content[0]
                    if hasattr(first, 'text'):
                        return first.text
            
            return str(result)
            
        except Exception as e:
            print(f"âŒ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def process_query(self, query: str):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã‚’å‡¦ç†"""
        print(f"\nğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {query}")
        
        # ãƒ„ãƒ¼ãƒ«æƒ…å ±ãŒæœªåé›†ã®å ´åˆã¯åé›†
        if not self.tools_schema:
            await self.collect_all_tools()
        
        # LLMã«ãƒ„ãƒ¼ãƒ«é¸æŠã‚’ä¾é ¼
        selection = await self.select_tool_with_llm(query)
        
        if not selection:
            print("ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        # ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
        server = selection.get("server")
        tool = selection.get("tool")
        arguments = selection.get("arguments", {})
        reasoning = selection.get("reasoning", "")
        
        # LLMãŒèª¤ã£ãŸå½¢å¼ã§è¿”ã—ãŸå ´åˆã®ä¿®æ­£
        # ã‚±ãƒ¼ã‚¹1: server.server.tool -> server, tool
        # ã‚±ãƒ¼ã‚¹2: server_name.server.tool -> server, tool
        if tool and '.' in tool:
            parts = tool.split('.')
            # æœ€å¾Œã®éƒ¨åˆ†ã‚’ãƒ„ãƒ¼ãƒ«åã¨ã—ã¦ä½¿ç”¨
            tool = parts[-1]
            # serveråã‚‚ä¿®æ­£ãŒå¿…è¦ãªå ´åˆ
            if '.' in server:
                server_parts = server.split('.')
                # calculator_server -> calculator ã®ã‚ˆã†ãªä¿®æ­£
                for known_server in self.servers.keys():
                    if known_server in server_parts[-1] or known_server in server_parts[0]:
                        server = known_server
                        break
        
        # serveråã«ä½™è¨ˆãªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ä¿®æ­£
        if server and server not in self.servers:
            # calculator_server -> calculator ã®ã‚ˆã†ãªä¿®æ­£
            for known_server in self.servers.keys():
                if known_server in server or server in known_server:
                    server = known_server
                    break
        
        if reasoning:
            print(f"ğŸ’­ åˆ¤æ–­: {reasoning}")
        
        print(f"ğŸ”§ å®Ÿè¡Œ: {server}.{tool} {arguments}")
        print(f"   [ãƒ‡ãƒãƒƒã‚°] ã‚µãƒ¼ãƒãƒ¼: {server}, ãƒ„ãƒ¼ãƒ«: {tool}")
        
        result = await self.execute_tool(server, tool, arguments)
        
        if result:
            # çµæœã‚’LLMã«è§£é‡ˆã•ã›ã¦å›ç­”
            interpretation_prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query}
å®Ÿè¡Œã—ãŸãƒ„ãƒ¼ãƒ«: {server}.{tool}
çµæœ: {result}

ã“ã®çµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åˆ†ã‹ã‚Šã‚„ã™ãæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
æ•°å€¤ã¯é©åˆ‡ã«ä¸¸ã‚ã¦ã€è‡ªç„¶ãªè¡¨ç¾ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
"""
            
            response = await self.llm.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that interprets tool results for users in Japanese."},
                    {"role": "user", "content": interpretation_prompt}
                ],
                temperature=0.7
            )
            
            print(f"ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {response.choices[0].message.content}")
        else:
            print("ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    
    async def interactive_session(self):
        """å¯¾è©±å‹ã‚»ãƒƒã‚·ãƒ§ãƒ³"""
        print("\n" + "="*50)
        print("ğŸ¤– LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆç°¡ç•¥ç‰ˆï¼‰")
        print("="*50)
        print("è‡ªç„¶è¨€èªã§è³ªå•ã—ã¦ãã ã•ã„ã€‚'exit'ã§çµ‚äº†ã—ã¾ã™ã€‚\n")
        
        # åˆå›ã®ãƒ„ãƒ¼ãƒ«åé›†
        await self.collect_all_tools()
        
        while True:
            try:
                query = input("\nğŸ’¬ > ")
                
                if query.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                    break
                
                if not query.strip():
                    continue
                
                await self.process_query(query)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        for client in self.clients.values():
            await client.__aexit__(None, None, None)

async def main():
    # APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    client = SimpleLLMClient()
    
    try:
        await client.interactive_session()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())