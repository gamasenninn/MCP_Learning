#!/usr/bin/env python3
"""
LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå®Œå…¨ç‰ˆï¼‰
Step 1-3ã®æˆæœã‚’çµ±åˆã—ãŸå®Ÿç”¨çš„ãªå¯¾è©±å‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
"""

import asyncio
import os
import sys

# Windowsç’°å¢ƒã§ã®Unicodeå¯¾å¿œ
if sys.platform == "win32":
    os.environ["PYTHONIOENCODING"] = "utf-8"
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv
from openai import AsyncOpenAI
from fastmcp import Client

# Step 1-3ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from mcp_llm_step1 import ToolCollector
from mcp_llm_step2 import LLMIntegrationPrep

load_dotenv()

class CompleteLLMClient:
    """å®Œå…¨ãªLLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        # Step 1-3ã®ã‚¯ãƒ©ã‚¹ã‚’æ´»ç”¨
        self.collector = ToolCollector()
        self.prep = LLMIntegrationPrep()
        self.llm = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç†
        self.clients = {}
        
        # ä¼šè©±å±¥æ­´ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        self.conversation_history = []
        self.context = {
            "session_start": datetime.now(),
            "tool_calls": 0,
            "errors": 0
        }
        
    async def initialize(self):
        """åˆæœŸåŒ–å‡¦ç†"""
        print("[èµ·å‹•] LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’èµ·å‹•ä¸­...", flush=True)
        
        # Step 1: ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’åé›†
        await self.collector.collect_all_tools()
        
        # MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æ¥ç¶š
        for server_name, server_info in self.collector.servers.items():
            try:
                client = Client(server_info["path"])
                await client.__aenter__()
                self.clients[server_name] = client
            except Exception as e:
                print(f"  âš ï¸ {server_name}ã¸ã®æ¥ç¶šå¤±æ•—: {e}")
        
        print("[å®Œäº†] åˆæœŸåŒ–å®Œäº†\n", flush=True)
        self._show_available_tools()
    
    def _show_available_tools(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’è¡¨ç¤º"""
        total_tools = sum(len(tools) for tools in self.collector.tools_schema.values())
        print(f"[ãƒ„ãƒ¼ãƒ«] åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«: {total_tools}å€‹")
        for server_name, tools in self.collector.tools_schema.items():
            print(f"  - {server_name}: {len(tools)}å€‹ã®ãƒ„ãƒ¼ãƒ«")
        print()
    
    async def _analyze_query(self, query: str) -> Dict:
        """ã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã€ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã®å¿…è¦æ€§ã¨å¯¾å¿œã‚’æ±ºå®š"""
        tools_desc = self.prep.prepare_tools_for_llm(self.collector.tools_schema)
        
        # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€å¤§5ä»¶ï¼‰
        recent_history = ""
        if self.conversation_history:
            recent_messages = self.conversation_history[-5:]
            history_lines = []
            for msg in recent_messages:
                role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
                history_lines.append(f"{role}: {msg['content']}")
            recent_history = "\n".join(history_lines)
        
        prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æã—ã€é©åˆ‡ãªå¯¾å¿œã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚

## ã“ã‚Œã¾ã§ã®ä¼šè©±
{recent_history if recent_history else "ï¼ˆæ–°ã—ã„ä¼šè©±ï¼‰"}

## ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{query}

## åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«
{tools_desc}

## åˆ¤å®šåŸºæº–
- è¨ˆç®—ã€ãƒ‡ãƒ¼ã‚¿å–å¾—ã€å¤–éƒ¨æƒ…å ±ã®å‚ç…§ã€ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡ŒãŒå¿…è¦ â†’ needs_tool: true
- ä¸€èˆ¬çš„ãªçŸ¥è­˜ã€èª¬æ˜ã€ä¼šè©±ã€æ„è¦‹ã€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ç­”ãˆã‚‰ã‚Œã‚‹ â†’ needs_tool: false
- é‡è¦ï¼šã“ã‚Œã¾ã§ã®ä¼šè©±ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„

## å¿œç­”å½¢å¼
ä»¥ä¸‹ã®JSONå½¢å¼ã§å¿…ãšå¿œç­”ã—ã¦ãã ã•ã„ï¼ˆJSONã®ã¿ã€èª¬æ˜æ–‡ã¯ä¸è¦ï¼‰ï¼š

needs_tool=trueã®å ´åˆ:
{{
  "needs_tool": true,
  "server": "ã‚µãƒ¼ãƒãƒ¼åã®ã¿ï¼ˆä¾‹: calculatorï¼‰",
  "tool": "ãƒ„ãƒ¼ãƒ«åã®ã¿ï¼ˆä¾‹: addï¼‰â€»ã‚µãƒ¼ãƒãƒ¼åã¯å«ã‚ãªã„",
  "arguments": {{ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿}},
  "reasoning": "ãªãœã“ã®ãƒ„ãƒ¼ãƒ«ã‚’é¸ã‚“ã ã‹"
}}

needs_tool=falseã®å ´åˆ:
{{
  "needs_tool": false,
  "reasoning": "ãªãœãƒ„ãƒ¼ãƒ«ãŒä¸è¦ã‹",
  "response": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ç›´æ¥å›ç­”"
}}

## é‡è¦ãªæ³¨æ„
- ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã¯ "ã‚µãƒ¼ãƒãƒ¼å.ãƒ„ãƒ¼ãƒ«å" ã®å½¢å¼ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ãŒ
- JSONã§ã¯ server ã¨ tool ã‚’åˆ¥ã€…ã«æŒ‡å®šã—ã¦ãã ã•ã„
- ä¾‹: "calculator.add" â†’ server: "calculator", tool: "add"
- ä¾‹: "weather.get_weather" â†’ server: "weather", tool: "get_weather"
"""
        
        response = await self.llm.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that analyzes queries and determines appropriate actions. Always respond with valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=0
        )
        
        # ãƒ‡ãƒãƒƒã‚°: LLMã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
        raw_response = response.choices[0].message.content
        print(f"  [LLM] ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæœ€åˆã®300æ–‡å­—ï¼‰:", flush=True)
        print(f"  {raw_response[:300]}...", flush=True)
        
        try:
            return self.prep.validate_llm_response(raw_response)
        except Exception as e:
            print(f"  âŒ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"  ğŸ“ å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹:")
            print(raw_response)
            raise
    
    async def process_query(self, query: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã‚’å‡¦ç†"""
        try:
            # ã‚¯ã‚¨ãƒªã‚’åˆ†æï¼ˆä¼šè©±å±¥æ­´ã‚’å‚ç…§ã—ã¤ã¤ï¼‰
            print("  [åˆ†æ] ã‚¯ã‚¨ãƒªã‚’åˆ†æä¸­...", flush=True)
            decision = await self._analyze_query(query)
            
            # åˆ†æå¾Œã«ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({"role": "user", "content": query})
            
            # åˆ¤æ–­ç†ç”±ã‚’è¡¨ç¤º
            if decision.get("reasoning"):
                print(f"  [åˆ¤æ–­] {decision['reasoning']}", flush=True)
            
            if decision.get("needs_tool", False):
                # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ‘ã‚¹
                print(f"  [é¸æŠ] ãƒ„ãƒ¼ãƒ«: {decision['server']}.{decision['tool']}", flush=True)
                print(f"     å¼•æ•°: {decision['arguments']}", flush=True)
                print(f"  [å®Ÿè¡Œ] å‡¦ç†ä¸­...", flush=True)
                
                result = await self._execute_tool(
                    decision['server'],
                    decision['tool'],
                    decision['arguments']
                )
                print(f"  [å®Œäº†] å®Ÿè¡Œå®Œäº†", flush=True)
                
                # çµæœã‚’è§£é‡ˆ
                print("  [è§£é‡ˆ] çµæœã‚’è§£é‡ˆä¸­...", flush=True)
                return await self._interpret_result(query, decision, result)
            else:
                # ç›´æ¥å¿œç­”ãƒ‘ã‚¹
                print("  [å¿œç­”] ç›´æ¥å¿œç­”ãƒ¢ãƒ¼ãƒ‰", flush=True)
                response = decision.get("response", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                self.conversation_history.append({"role": "assistant", "content": response})
                return response
                
        except Exception as e:
            self.context["errors"] += 1
            return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    async def _execute_tool(self, server: str, tool: str, arguments: Dict) -> Any:
        """MCPãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
        if server not in self.clients:
            raise ValueError(f"ã‚µãƒ¼ãƒãƒ¼ '{server}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        self.context["tool_calls"] += 1
        client = self.clients[server]
        result = await client.call_tool(tool, arguments)
        
        # çµæœã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        if hasattr(result, 'content'):
            if isinstance(result.content, list) and result.content:
                first = result.content[0]
                if hasattr(first, 'text'):
                    return first.text
        return str(result)
    
    async def _interpret_result(self, query: str, selection: Dict, result: Any) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’è‡ªç„¶è¨€èªã§è§£é‡ˆ"""
        interpretation_prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query}
å®Ÿè¡Œã—ãŸãƒ„ãƒ¼ãƒ«: {selection['server']}.{selection['tool']}
å¼•æ•°: {selection['arguments']}
çµæœ: {result}

ã“ã®çµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦åˆ†ã‹ã‚Šã‚„ã™ãæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
æ•°å€¤ã¯é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã€æŠ€è¡“çš„ãªè©³ç´°ã¯å¿…è¦æœ€å°é™ã«ã—ã¦ãã ã•ã„ã€‚
"""
        
        response = await self.llm.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": interpretation_prompt}
            ],
            temperature=0.7
        )
        
        answer = response.choices[0].message.content
        self.conversation_history.append({"role": "assistant", "content": answer})
        return answer
    
    async def _generate_conversation_response(self, query: str) -> str:
        """é€šå¸¸ã®ä¼šè©±å¿œç­”ã‚’ç”Ÿæˆ"""
        # ä¼šè©±å±¥æ­´ã‚’å«ã‚ã¦å¿œç­”
        messages = [
            {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"}
        ]
        
        # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€å¤§10ä»¶ï¼‰
        recent_history = self.conversation_history[-10:] if len(self.conversation_history) > 10 else self.conversation_history
        messages.extend(recent_history)
        
        response = await self.llm.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7
        )
        
        answer = response.choices[0].message.content
        self.conversation_history.append({"role": "assistant", "content": answer})
        return answer
    
    async def interactive_session(self):
        """å¯¾è©±å‹ã‚»ãƒƒã‚·ãƒ§ãƒ³"""
        print("="*60)
        print("[LLMçµ±åˆMCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ] å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
        print("="*60)
        print("[ãƒ’ãƒ³ãƒˆ]")
        print("  - è‡ªç„¶ãªæ—¥æœ¬èªã§è³ªå•ã—ã¦ãã ã•ã„")
        print("  - 'help'ã§ãƒ˜ãƒ«ãƒ—è¡¨ç¤º")
        print("  - 'tools'ã§åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§")
        print("  - 'history'ã§ä¼šè©±å±¥æ­´")
        print("  - 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†")
        print("-"*60 + "\n")
        
        while True:
            try:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¡¨ç¤º
                user_input = input("You> ").strip()
                
                # ç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰å‡¦ç†
                if user_input.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("\n[çµ‚äº†] ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                elif user_input.lower() == 'help':
                    self._show_help()
                    continue
                elif user_input.lower() == 'tools':
                    self._show_available_tools()
                    continue
                elif user_input.lower() == 'history':
                    self._show_history()
                    continue
                elif not user_input:
                    continue
                
                # ã‚¯ã‚¨ãƒªã‚’å‡¦ç†
                print("\n" + "="*40, flush=True)
                response = await self.process_query(user_input)
                print("-"*40)
                print(f"\nAssistant> {response}\n", flush=True)
                
            except KeyboardInterrupt:
                print("\n\n[ä¸­æ–­] ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"\n[ã‚¨ãƒ©ãƒ¼] {e}\n")
    
    def _show_help(self):
        """ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º"""
        print("\n[ãƒ˜ãƒ«ãƒ—]")
        print("  è¨ˆç®—ä¾‹: '100ã¨250ã‚’è¶³ã—ã¦'")
        print("  å¤©æ°—ä¾‹: 'æ±äº¬ã®å¤©æ°—ã‚’æ•™ãˆã¦'")
        print("  DBä¾‹: 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¦'")
        print("  ä¼šè©±ä¾‹: 'MCPã«ã¤ã„ã¦æ•™ãˆã¦'")
        print()
    
    def _show_history(self):
        """ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤º"""
        print("\n[å±¥æ­´] ä¼šè©±å±¥æ­´:")
        if not self.conversation_history:
            print("  ï¼ˆã¾ã ä¼šè©±ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
        else:
            for i, msg in enumerate(self.conversation_history[-10:], 1):
                role = "You" if msg["role"] == "user" else "AI"
                content = msg["content"][:100] + "..." if len(msg["content"]) > 100 else msg["content"]
                print(f"  {i}. {role}: {content}")
        print()
    
    def show_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        duration = datetime.now() - self.context["session_start"]
        print("\n[çµ±è¨ˆ] ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
        print(f"  - ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“: {duration}")
        print(f"  - ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—: {self.context['tool_calls']}å›")
        print(f"  - ã‚¨ãƒ©ãƒ¼: {self.context['errors']}å›")
        print(f"  - ä¼šè©±æ•°: {len(self.conversation_history)}ä»¶")
    
    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
        for client in self.clients.values():
            try:
                await client.__aexit__(None, None, None)
            except:
                pass

async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("[ã‚¨ãƒ©ãƒ¼] ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        print("   ä¾‹: export OPENAI_API_KEY='your-api-key'")
        return
    
    client = CompleteLLMClient()
    
    try:
        # åˆæœŸåŒ–
        await client.initialize()
        
        # å¯¾è©±å‹ã‚»ãƒƒã‚·ãƒ§ãƒ³
        await client.interactive_session()
        
        # çµ±è¨ˆè¡¨ç¤º
        client.show_statistics()
        
    finally:
        await client.cleanup()
        print("\n[çµ‚äº†] ã”åˆ©ç”¨ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n[çµ‚äº†] ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
        sys.exit(0)