"""
Step 2: LLMçµ±åˆã®æº–å‚™ (V2 - mcpServerså½¢å¼å¯¾å¿œ)
ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’LLMãŒç†è§£ã—ã‚„ã™ã„å½¢å¼ã«æ•´å½¢
"""
import json
from typing import Dict, List, Any

class LLMIntegrationPrepV2:
    """LLMçµ±åˆã®ãŸã‚ã®æº–å‚™ã‚¯ãƒ©ã‚¹ï¼ˆmcpServerså½¢å¼å¯¾å¿œï¼‰"""
    
    def prepare_tools_for_llm(self, tools_schema: Dict[str, List[Any]]) -> str:
        """ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’LLMç”¨ã«æ•´å½¢
        
        é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼š
        1. å„ãƒ„ãƒ¼ãƒ«ã®å½¹å‰²ã‚’æ˜ç¢ºã«è¨˜è¿°
        2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹ã¨å¿…é ˆ/ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ˜ç¤º
        3. å…·ä½“çš„ãªä½¿ç”¨ä¾‹ã‚’æä¾›
        4. mcpServerså½¢å¼ã®ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’é©åˆ‡ã«å‡¦ç†
        """
        tools_description = []
        
        for server_name, tools in tools_schema.items():
            for tool in tools:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°èª¬æ˜ã‚’ç”Ÿæˆ
                params_desc = self._format_parameters(tool.get('parameters', {}))
                
                tool_desc = f"""
{server_name}.{tool['name']}:
  èª¬æ˜: {tool['description']}
  {params_desc}
"""
                tools_description.append(tool_desc.strip())
        
        return "\n\n".join(tools_description)
    
    def _format_parameters(self, parameters: Dict) -> str:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±ã‚’èª­ã¿ã‚„ã™ãæ•´å½¢ï¼ˆV2å½¢å¼å¯¾å¿œï¼‰"""
        if not parameters:
            return "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: ãªã—"
        
        param_lines = ["ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:"]
        
        for key, value in parameters.items():
            param_type = value.get('type', 'any')
            param_desc = value.get('description', '')
            
            # V2å½¢å¼ã§ã¯å¿…é ˆ/ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®åˆ¤å®šã‚’ç°¡ç•¥åŒ–
            param_lines.append(f"    - {key} ({param_type}): {param_desc}")
        
        return "\n  ".join(param_lines)
    
    def create_tool_selection_prompt(self, user_query: str, tools_desc: str) -> str:
        """LLMã«ãƒ„ãƒ¼ãƒ«é¸æŠã‚’ä¾é ¼ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        return f"""ã‚ãªãŸã¯MCPãƒ„ãƒ¼ãƒ«é¸æŠã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
{tools_desc}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: "{user_query}"

ã“ã®è¦æ±‚ã«æœ€é©ãªãƒ„ãƒ¼ãƒ«ã‚’é¸æŠã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„ï¼š

{{
  "selected_tool": "ã‚µãƒ¼ãƒãƒ¼å.ãƒ„ãƒ¼ãƒ«å",
  "parameters": {{
    "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å": "å€¤"
  }},
  "reasoning": "é¸æŠç†ç”±"
}}

é‡è¦ãªæ³¨æ„ç‚¹ï¼š
- å¿…ãšJSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„
- åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‹ã‚‰é©åˆ‡ãªã‚‚ã®ã‚’é¸ã‚“ã§ãã ã•ã„
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹ã¨å€¤ã«æ³¨æ„ã—ã¦ãã ã•ã„
- æ•°å€¤ãŒå¿…è¦ãªå ´åˆã¯é©åˆ‡ãªæ•°å€¤å‹ã§æŒ‡å®šã—ã¦ãã ã•ã„"""
    
    def validate_llm_response(self, response_text: str) -> Dict:
        """LLMã®å¿œç­”ã‚’æ¤œè¨¼ãƒ»ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ã®å ´åˆã‚‚å¯¾å¿œï¼‰
            response_text = response_text.strip()
            if response_text.startswith("```"):
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ã®å ´åˆã€ä¸­èº«ã‚’å–ã‚Šå‡ºã™
                lines = response_text.split('\n')
                json_lines = []
                in_code_block = False
                for line in lines:
                    if line.startswith("```"):
                        in_code_block = not in_code_block
                        continue
                    if in_code_block:
                        json_lines.append(line)
                response_text = '\n'.join(json_lines)
            
            parsed = json.loads(response_text)
            
            # å¿…è¦ãªã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
            required_keys = ["selected_tool", "parameters", "reasoning"]
            for key in required_keys:
                if key not in parsed:
                    return {
                        "valid": False,
                        "error": f"å¿…è¦ãªã‚­ãƒ¼ '{key}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                    }
            
            # ãƒ„ãƒ¼ãƒ«åã®å½¢å¼ç¢ºèªï¼ˆã‚µãƒ¼ãƒãƒ¼å.ãƒ„ãƒ¼ãƒ«åï¼‰
            tool_name = parsed["selected_tool"]
            if "." not in tool_name:
                return {
                    "valid": False,
                    "error": "ãƒ„ãƒ¼ãƒ«åã¯ 'ã‚µãƒ¼ãƒãƒ¼å.ãƒ„ãƒ¼ãƒ«å' ã®å½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                }
            
            server_name, tool_name_part = tool_name.split(".", 1)
            
            return {
                "valid": True,
                "server_name": server_name,
                "tool_name": tool_name_part,
                "parameters": parsed["parameters"],
                "reasoning": parsed["reasoning"]
            }
            
        except json.JSONDecodeError as e:
            return {
                "valid": False,
                "error": f"JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}"
            }
        except Exception as e:
            return {
                "valid": False,
                "error": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"
            }
    
    def display_llm_integration_example(self):
        """LLMçµ±åˆã®ä¾‹ã‚’è¡¨ç¤º"""
        print("ğŸ¤– LLMçµ±åˆã®ä¾‹:")
        print("\nãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: 'æ±äº¬ã®å¤©æ°—ã‚’æ•™ãˆã¦'")
        print("\nLLMã®å¿œç­”ä¾‹:")
        example_response = {
            "selected_tool": "weather.get_weather",
            "parameters": {
                "city": "Tokyo"
            },
            "reasoning": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ±äº¬ã®å¤©æ°—æƒ…å ±ã‚’æ±‚ã‚ã¦ã„ã‚‹ãŸã‚ã€weather ã‚µãƒ¼ãƒãƒ¼ã® get_weather ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™"
        }
        print(json.dumps(example_response, ensure_ascii=False, indent=2))
        
        print("\nâœ… æ¤œè¨¼çµæœ: æœ‰åŠ¹ãªå¿œç­”")
        validation = self.validate_llm_response(json.dumps(example_response))
        if validation["valid"]:
            print(f"   ã‚µãƒ¼ãƒãƒ¼: {validation['server_name']}")
            print(f"   ãƒ„ãƒ¼ãƒ«: {validation['tool_name']}")
            print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {validation['parameters']}")

# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨
if __name__ == "__main__":
    prep = LLMIntegrationPrepV2()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ„ãƒ¼ãƒ«ã‚¹ã‚­ãƒ¼ãƒï¼ˆmcpServerså½¢å¼å¯¾å¿œï¼‰
    sample_schema = {
        "calculator": [
            {
                "name": "add",
                "description": "2ã¤ã®æ•°å€¤ã‚’åŠ ç®—ã—ã¾ã™",
                "parameters": {
                    "a": {"type": "number", "description": "ç¬¬1ã®æ•°å€¤"},
                    "b": {"type": "number", "description": "ç¬¬2ã®æ•°å€¤"}
                }
            }
        ],
        "weather": [
            {
                "name": "get_weather",
                "description": "æŒ‡å®šã—ãŸéƒ½å¸‚ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—ã—ã¾ã™",
                "parameters": {
                    "city": {"type": "string", "description": "éƒ½å¸‚å"}
                }
            }
        ]
    }
    
    print("ğŸš€ LLMçµ±åˆæº–å‚™ã‚·ã‚¹ãƒ†ãƒ  (V2 - mcpServerså½¢å¼å¯¾å¿œ)")
    print("="*50)
    
    # ãƒ„ãƒ¼ãƒ«æƒ…å ±ã®æ•´å½¢ä¾‹
    formatted = prep.prepare_tools_for_llm(sample_schema)
    print("\nğŸ“‹ æ•´å½¢ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«æƒ…å ±:")
    print(formatted)
    
    # LLMçµ±åˆä¾‹ã®è¡¨ç¤º
    prep.display_llm_integration_example()