#!/usr/bin/env python3
"""
MCP Agent V4 - Interactive Dialogue Engine
Claude Codeé¢¨ã®å¯¾è©±å‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

V4ã®ç‰¹å¾´ï¼š
- å¯¾è©±çš„é€æ¬¡å®Ÿè¡Œï¼ˆä¾å­˜é–¢ä¿‚ã®è‡ªå‹•è§£æ±ºï¼‰
- ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ä»˜ãã‚¿ã‚¹ã‚¯è¡¨ç¤º
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹
- V3ã®çŸ¥è¦‹ã‚’æ´»ã‹ã—ãŸè¨­è¨ˆ
"""

import os
import json
import asyncio
import time
import yaml
from typing import Dict, List, Any, Optional
from datetime import datetime
from openai import AsyncOpenAI

from connection_manager import ConnectionManager
from display_manager import DisplayManager
from error_handler import ErrorHandler
from prompts import PromptTemplates
from utils import Logger, safe_str

# Rich UI support
try:
    from display_manager_rich import RichDisplayManager
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False




class MCPAgentV4:
    """
    Claude Codeé¢¨ã®å¯¾è©±å‹MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    V3ã‹ã‚‰å¼•ãç¶™ã„ã è¦ç´ :
    - AGENT.mdã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    - ä¼šè©±æ–‡è„ˆã®æ´»ç”¨
    - NO_TOOLåˆ¤å®š
    
    V4ã®æ–°æ©Ÿèƒ½:
    - å¯¾è©±çš„é€æ¬¡å®Ÿè¡Œ
    - ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®å¯è¦–åŒ–
    - ä¾å­˜é–¢ä¿‚ã®è‡ªå‹•è§£æ±º
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """åˆæœŸåŒ–"""
        self.config = self._load_config(config_path)
        
        # UI ãƒ¢ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦é©åˆ‡ãªDisplayManagerã‚’é¸æŠ
        ui_mode = self.config.get("display", {}).get("ui_mode", "basic")
        
        if ui_mode == "rich" and RICH_AVAILABLE:
            self.display = RichDisplayManager(
                show_timing=self.config["display"]["show_timing"],
                show_thinking=self.config["display"]["show_thinking"]
            )
            self.ui_mode = "rich"
        else:
            if ui_mode == "rich" and not RICH_AVAILABLE:
                print("[WARNING] Rich UI requested but rich library not available. Using basic UI.")
            self.display = DisplayManager(
                show_timing=self.config["display"]["show_timing"],
                show_thinking=self.config["display"]["show_thinking"]
            )
            self.ui_mode = "basic"
        
        # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.llm = AsyncOpenAI()
        
        # MCPæ¥ç¶šç®¡ç†ï¼ˆV3ã‹ã‚‰æµç”¨ï¼‰
        self.connection_manager = ConnectionManager()
        
        # ã‚¨ãƒ©ãƒ¼å‡¦ç†å¸ä»¤å¡”ï¼ˆV4æ–°æ©Ÿèƒ½ï¼‰
        self.error_handler = ErrorHandler(
            config=self.config,
            llm=self.llm,
            verbose=self.config.get("development", {}).get("verbose", True)
        )
        
        # ä¼šè©±å±¥æ­´ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
        self.conversation_history: List[Dict] = []
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
        self.session_stats = {
            "start_time": datetime.now(),
            "total_requests": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_api_calls": 0
        }
        
        # å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        self.execution_metrics = {
            "task_generation_success": 0,
            "task_generation_failures": 0,
            "task_generation_retry_success": 0,
            "task_generation_total_failures": 0,
            "json_parse_errors": 0,
            "timeout_count": 0,
            "fallback_usage": 0,
            "average_task_count": 0.0,
            "total_task_lists": 0
        }
        
        # AGENT.mdèª­ã¿è¾¼ã¿ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
        self.custom_instructions = self._load_agent_md()
        
        # Loggerã‚’åˆæœŸåŒ–
        self.verbose = self.config.get("development", {}).get("verbose", True)
        self.logger = Logger(verbose=self.verbose)
        
        if self.verbose:
            self.display.show_banner()
            if self.ui_mode == "rich":
                self.logger.info("Rich UI mode enabled")
            else:
                self.logger.info("Basic UI mode enabled")
    
    def _load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆå¿…é ˆï¼‰"""
        if not os.path.exists(config_path):
            raise FileNotFoundError(
                f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                f"'config.sample.yaml' ã‚’ '{config_path}' ã«ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚"
            )
        
        with open(config_path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    
    def _load_agent_md(self) -> str:
        """AGENT.mdã‚’èª­ã¿è¾¼ã¿ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰"""
        agent_md_path = "AGENT.md"
        
        if os.path.exists(agent_md_path):
            try:
                with open(agent_md_path, "r", encoding="utf-8") as f:
                    content = f.read()
                if hasattr(self, 'logger'):
                    self.logger.config_info(f"AGENT.mdã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(content)}æ–‡å­—)")
                elif self.config.get("development", {}).get("verbose", False):
                    print(f"[è¨­å®š] AGENT.mdã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(content)}æ–‡å­—)")
                return content
            except Exception as e:
                print(f"[è­¦å‘Š] AGENT.mdèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return ""
        else:
            if self.config.get("development", {}).get("verbose", False):
                print("[æƒ…å ±] AGENT.mdãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆåŸºæœ¬èƒ½åŠ›ã®ã¿ã§å‹•ä½œï¼‰")
            return ""
    
    async def initialize(self):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–"""
        if self.verbose:
            print(f"\n[æŒ‡ç¤ºæ›¸] {'ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤ºã‚ã‚Š' if self.custom_instructions else 'åŸºæœ¬èƒ½åŠ›ã®ã¿'}")
            print("=" * 60)
        
        # MCPæ¥ç¶šç®¡ç†ã‚’åˆæœŸåŒ–ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
        await self.connection_manager.initialize()
    
    async def process_request(self, user_query: str) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å¯¾è©±çš„ã«å‡¦ç†ï¼ˆV4ã®æ ¸å¿ƒæ©Ÿèƒ½ï¼‰
        
        V3ã¨ã®é•ã„:
        - ä¸€åº¦ã«å…¨ã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£ã›ãšã€ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å¯¾è©±
        - å‰ã®çµæœã‚’è¦‹ã¦ã‹ã‚‰æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š
        - å®Ÿè¡Œéç¨‹ã‚’è¦–è¦šçš„ã«è¡¨ç¤º
        """
        self.session_stats["total_requests"] += 1
        
        if self.verbose:
            print(f"\n[ãƒªã‚¯ã‚¨ã‚¹ãƒˆ #{self.session_stats['total_requests']}] {user_query}")
            print("-" * 60)
        
        # ä¼šè©±æ–‡è„ˆã‚’è¡¨ç¤º
        context_count = min(len(self.conversation_history), 
                          self.config["conversation"]["context_limit"])
        if context_count > 0:
            self.display.show_context_info(context_count)
        
        try:
            # å¯¾è©±çš„å®Ÿè¡Œã®é–‹å§‹
            response = await self._execute_interactive_dialogue(user_query)
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
            # å®Ÿè¡Œçµæœã«ã¤ã„ã¦ã¯å„å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ã§è¿½åŠ ã•ã‚Œã‚‹
            self._add_to_history("user", user_query)
            
            return response
            
        except Exception as e:
            error_msg = f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            if self.verbose:
                print(f"[ã‚¨ãƒ©ãƒ¼] {error_msg}")
            return error_msg
    
    async def _execute_interactive_dialogue(self, user_query: str) -> str:
        """
        æ”¹è‰¯ç‰ˆå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆV4.1ï¼‰
        
        è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã¯ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆæ–¹å¼ã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ã¯å¾“æ¥æ–¹å¼
        """
        # ç¾åœ¨ã®ã‚¯ã‚¨ãƒªã‚’ä¿å­˜ï¼ˆLLMåˆ¤æ–­ã§ä½¿ç”¨ï¼‰
        self.current_user_query = user_query
        self.display.show_analysis("ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æä¸­...")
        
        # ã¾ãšå‡¦ç†æ–¹å¼ã‚’åˆ¤å®š
        execution_result = await self._determine_execution_type(user_query)
        execution_type = execution_result.get("type", "SIMPLE")
        
        if execution_type == "NO_TOOL":
            response = execution_result.get("response", "äº†è§£ã—ã¾ã—ãŸã€‚")
            self._add_to_history("assistant", response)
            return response
        else:
            # SIMPLE/COMPLEXçµ±åˆï¼šå…¨ã¦ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œè¦æ±‚ã‚’çµ±ä¸€ãƒ¡ã‚½ãƒƒãƒ‰ã§å‡¦ç†
            return await self._execute_with_tasklist(user_query)
    
    async def _determine_execution_type(self, user_query: str) -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ãŒNO_TOOLã‹ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‹ã‚’åˆ¤å®šï¼ˆSIMPLE/COMPLEXçµ±åˆå¾Œï¼‰"""
        recent_context = self._get_recent_context()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å–å¾—
        prompt = PromptTemplates.get_execution_type_determination_prompt(
            recent_context=recent_context,
            user_query=user_query
        )

        try:
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "system", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            content = safe_str(response.choices[0].message.content)
            result = json.loads(content)
            
            # SIMPLE/COMPLEXçµ±åˆã®ãŸã‚ã€NO_TOOLä»¥å¤–ã¯å…¨ã¦TOOLã«çµ±ä¸€
            if result.get('type') in ['SIMPLE', 'COMPLEX']:
                result['type'] = 'TOOL'
            
            self.logger.info(f"åˆ¤å®š: {result.get('type', 'UNKNOWN')} - {result.get('reason', '')}")
            
            return result
            
        except Exception as e:
            print(f"[ã‚¨ãƒ©ãƒ¼] å®Ÿè¡Œæ–¹å¼åˆ¤å®šå¤±æ•—: {e}")
            return {"type": "TOOL", "reason": "åˆ¤å®šã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ"}
    
    async def _execute_with_tasklist(self, user_query: str) -> str:
        """çµ±ä¸€ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆSIMPLE/COMPLEXçµ±åˆç‰ˆï¼‰"""
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆï¼ˆçµ±ä¸€ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
        task_list = await self._generate_task_list_with_retry(user_query)
        
        if not task_list:
            # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            error_msg = (f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚{user_query}ã®å‡¦ç†æ–¹æ³•ã‚’æ±ºå®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                       f"åˆ¥ã®è¡¨ç¾ã§å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            return error_msg
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        self.execution_metrics["total_task_lists"] += 1
        self.execution_metrics["average_task_count"] += len(task_list)
        
        # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º
        if self.ui_mode == "rich" and self.config.get("display", {}).get("rich_options", {}).get("enable_live_updates", True):
            self.display.show_checklist(task_list)
        else:
            self.display.show_checklist(task_list)
        
        # å®Ÿè¡Œçµæœã‚’è¿½è·¡
        completed = []
        failed = []
        execution_context = []
        
        # ã‚¿ã‚¹ã‚¯ã‚’é †æ¬¡å®Ÿè¡Œ
        for i, task in enumerate(task_list):
            # é€²è¡ŒçŠ¶æ³æ›´æ–°ï¼ˆRich UIã®å ´åˆã¯ãƒ©ã‚¤ãƒ–æ›´æ–°ï¼‰
            if self.ui_mode == "rich" and hasattr(self.display, 'update_checklist_live'):
                self.display.update_checklist_live(task_list, current=i, completed=completed, failed=failed)
            else:
                self.display.update_checklist(task_list, current=i, completed=completed, failed=failed)
            
            try:
                # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œï¼ˆã“ã‚Œã¾ã§ã®å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™ï¼‰
                result = await self._execute_planned_task(task, i+1, len(task_list), execution_context.copy())
                
                if result["success"]:
                    completed.append(i)
                    task["duration"] = result["duration"]
                else:
                    failed.append(i)
                
                execution_context.append(result)
                
            except Exception as e:
                failed.append(i)
                print(f"[ã‚¨ãƒ©ãƒ¼] ã‚¿ã‚¹ã‚¯{i+1}å®Ÿè¡Œå¤±æ•—: {e}")
        
        # æœ€çµ‚çŠ¶æ³è¡¨ç¤º
        if self.ui_mode == "rich" and hasattr(self.display, 'update_checklist_live'):
            self.display.update_checklist_live(task_list, current=-1, completed=completed, failed=failed)
        else:
            self.display.update_checklist(task_list, current=-1, completed=completed, failed=failed)
        
        # çµæœã‚’LLMã§è§£é‡ˆ
        return await self._interpret_planned_results(user_query, execution_context)
    
    def _resolve_placeholders(self, params: Dict, execution_context: List[Dict]) -> Dict:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†…ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å®Ÿéš›ã®å€¤ã«ç½®æ›
        
        ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼:
        - {{previous_result}} - ç›´å‰ã®ã‚¿ã‚¹ã‚¯çµæœ
        - {{task_N.field}} - Nç•ªç›®ã®ã‚¿ã‚¹ã‚¯ã®ç‰¹å®šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        - æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆä¾‹ï¼šã€Œå–å¾—ã—ãŸéƒ½å¸‚åã€â†’ å®Ÿéš›ã®éƒ½å¸‚åï¼‰
        """
        if not execution_context:
            return params
        
        import re
        import json
        
        def replace_value(value):
            if not isinstance(value, str):
                return value
                
            # {{previous_result}} ãƒ‘ã‚¿ãƒ¼ãƒ³
            if value == "{{previous_result}}" and execution_context:
                last_result = execution_context[-1].get("result", "")
                return str(last_result)
            
            # {{task_N.field}} ãƒ‘ã‚¿ãƒ¼ãƒ³
            task_pattern = r'\{\{task_(\d+)\.(\w+)\}\}'
            matches = re.findall(task_pattern, value)
            for task_num, field in matches:
                task_index = int(task_num) - 1
                if 0 <= task_index < len(execution_context):
                    task_result = execution_context[task_index].get("result", {})
                    if isinstance(task_result, dict) and field in task_result:
                        placeholder = f"{{{{task_{task_num}.{field}}}}}"
                        value = value.replace(placeholder, str(task_result[field]))
            
            # æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆIPåœ°ç†æƒ…å ± â†’ å¤©æ°—ç”¨ï¼‰
            if value in ["å–å¾—ã—ãŸéƒ½å¸‚å", "å–å¾—ã—ãŸéƒ½å¸‚", "éƒ½å¸‚å"]:
                # æœ€æ–°ã®çµæœã‹ã‚‰éƒ½å¸‚æƒ…å ±ã‚’æ¢ã™
                for result_data in reversed(execution_context):
                    result = result_data.get("result", {})
                    if isinstance(result, dict):
                        # IPã‚¢ãƒ‰ãƒ¬ã‚¹æƒ…å ±ã‹ã‚‰éƒ½å¸‚ã‚’å–å¾—
                        if "city" in result:
                            return result["city"]
                        elif "å¸‚" in str(result) or "åŒº" in str(result):
                            # æ—¥æœ¬ã®å¸‚åŒºæƒ…å ±ã‚’æ¤œç´¢
                            city_match = re.search(r'([^ã€]+[å¸‚åŒº])', str(result))
                            if city_match:
                                return city_match.group(1)
            
            return value
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å„å€¤ã‚’å†å¸°çš„ã«å‡¦ç†
        def process_params(obj):
            if isinstance(obj, dict):
                return {k: process_params(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [process_params(item) for item in obj]
            else:
                return replace_value(obj)
        
        return process_params(params)
    
    async def _generate_adaptive_task_list(self, user_query: str, temperature: float = 0.1) -> List[Dict]:
        """
        ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã«å¿œã˜ã¦é©å¿œçš„ã«ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            temperature: LLMã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        recent_context = self._get_conversation_context_only()
        tools_info = self.connection_manager.format_tools_for_llm()
        
        # ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤ºã®æœ‰ç„¡ã§è¤‡é›‘ã•ã‚’åˆ¤å®š
        custom_instructions = self.custom_instructions if self.custom_instructions.strip() else None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å–å¾—
        prompt = PromptTemplates.get_adaptive_task_list_prompt(
            recent_context=recent_context,
            user_query=user_query,
            tools_info=tools_info,
            custom_instructions=custom_instructions
        )

        try:
            self.session_stats["total_api_calls"] += 1
            
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "system", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=temperature
            )
            
            content = safe_str(response.choices[0].message.content)
            result = json.loads(content)
            tasks = result.get("tasks", [])
            
            # ã‚¿ã‚¹ã‚¯æ•°ã«å¿œã˜ã¦ãƒ­ã‚°å‡ºåŠ›
            task_type_label = "è©³ç´°" if custom_instructions else "ã‚·ãƒ³ãƒ—ãƒ«"
            self.logger.info(f"{task_type_label}ã‚¿ã‚¹ã‚¯: {len(tasks)}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ")
            for i, task in enumerate(tasks, 1):
                self.logger.debug(f"  [{i}] Tool: {task.get('tool')}, Params: {safe_str(task.get('params', {}))[:100]}...")
                self.logger.debug(f"      Description: {task.get('description', 'N/A')}")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªå ´åˆã¯æœ€å¤§3ã‚¿ã‚¹ã‚¯ã«åˆ¶é™
            if not custom_instructions and len(tasks) > 3:
                self.logger.info(f"ã‚¿ã‚¹ã‚¯æ•°åˆ¶é™: {len(tasks)} â†’ 3ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãƒ¢ãƒ¼ãƒ‰ï¼‰")
                tasks = tasks[:3]
            
            return tasks
            
        except Exception as e:
            print(f"[ã‚¨ãƒ©ãƒ¼] é©å¿œçš„ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆå¤±æ•—: {e}")
            return []
    
    async def _generate_task_list_with_retry(self, user_query: str) -> List[Dict]:
        """
        ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãé©å¿œçš„ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        retry_config = self.config.get("execution", {}).get("retry_strategy", {})
        max_retries = retry_config.get("max_retries", 3)
        use_progressive = retry_config.get("progressive_temperature", True)
        initial_temp = retry_config.get("initial_temperature", 0.1)
        temp_increment = retry_config.get("temperature_increment", 0.2)
        
        last_error = None
        
        for attempt in range(max_retries):
            try:
                # ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–temperatureèª¿æ•´
                if use_progressive and attempt > 0:
                    temperature = min(initial_temp + (attempt * temp_increment), 0.9)
                else:
                    temperature = initial_temp
                
                # çµ±ä¸€ã•ã‚ŒãŸé©å¿œçš„ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆã‚’ä½¿ç”¨
                task_list = await self._generate_adaptive_task_list(user_query, temperature)
                
                if task_list:
                    # æˆåŠŸæ™‚ã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
                    if hasattr(self, 'execution_metrics'):
                        self.execution_metrics['task_generation_success'] += 1
                        if attempt > 0:
                            self.execution_metrics['task_generation_retry_success'] += 1
                    
                    if attempt > 0:
                        self.logger.info(f"[æˆåŠŸ] ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ - {attempt + 1}å›ç›®ã®è©¦è¡Œã§æˆåŠŸ")
                    
                    # ã‚¿ã‚¹ã‚¯æ•°åˆ¶é™ï¼ˆå…¨ä½“çš„ãªä¸Šé™ï¼‰
                    max_tasks = self.config.get("execution", {}).get("max_tasks", 10)
                    if len(task_list) > max_tasks:
                        self.logger.warning(f"ã‚¿ã‚¹ã‚¯æ•°åˆ¶é™: {len(task_list)} â†’ {max_tasks}")
                        task_list = task_list[:max_tasks]
                    
                    return task_list
                else:
                    last_error = f"è©¦è¡Œ{attempt + 1}: ç©ºã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ"
                    
            except json.JSONDecodeError as e:
                last_error = f"è©¦è¡Œ{attempt + 1}: JSONè§£æã‚¨ãƒ©ãƒ¼ - {str(e)}"
                self.logger.info(f"[ãƒªãƒˆãƒ©ã‚¤] {last_error}")
            except Exception as e:
                last_error = f"è©¦è¡Œ{attempt + 1}: {str(e)}"
                self.logger.info(f"[ãƒªãƒˆãƒ©ã‚¤] {last_error}")
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            if hasattr(self, 'execution_metrics'):
                self.execution_metrics['task_generation_failures'] += 1
        
        # å…¨ã¦ã®è©¦è¡ŒãŒå¤±æ•—
        self.logger.error(f"[å¤±æ•—] ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ - {max_retries}å›ã®è©¦è¡Œå…¨ã¦ãŒå¤±æ•—")
        self.logger.error(f"æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {last_error}")
        
        if hasattr(self, 'execution_metrics'):
            self.execution_metrics['task_generation_total_failures'] += 1
            
        return []

    
    
    async def _execute_tool_with_retry(self, tool: str, params: Dict, description: str = "") -> Any:
        """
        LLMãƒ™ãƒ¼ã‚¹ã®è³¢ã„ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ»åˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ 
        
        Args:
            tool: ãƒ„ãƒ¼ãƒ«å
            params: å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            description: ã‚¿ã‚¹ã‚¯ã®èª¬æ˜ï¼ˆLLMåˆ¤æ–­æ™‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        """
        self.logger.info(f"[DEBUG] _execute_tool_with_retry ãŒå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸ: tool={tool}")
        max_retries = 3
        
        # å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒï¼ˆç ´å£Šçš„å¤‰æ›´ã‚’é¿ã‘ã‚‹ï¼‰
        original_params = params.copy()
        current_params = params.copy()
        
        for attempt in range(max_retries + 1):
            # 1. ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œï¼ˆä¾‹å¤–ã‚‚ã‚­ãƒ£ãƒƒãƒã—ã¦çµæœã¨ã—ã¦æ‰±ã†ï¼‰
            try:
                raw_result = await self.connection_manager.call_tool(tool, current_params)
                self.logger.info(f"[DEBUG] ãƒ„ãƒ¼ãƒ«å®Ÿè¡ŒæˆåŠŸ attempt={attempt + 1}, result_type={type(raw_result)}")
            except Exception as e:
                # ä¾‹å¤–ã‚‚ã€Œçµæœã€ã¨ã—ã¦æ‰±ã„ã€LLMåˆ¤æ–­ã«å›ã™
                raw_result = f"ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}"
                self.logger.info(f"[DEBUG] ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ attempt={attempt + 1}, error={type(e).__name__}")
            
            # 2. LLMã«çµæœã‚’åˆ¤æ–­ã•ã›ã‚‹ï¼ˆæˆåŠŸãƒ»å¤±æ•—å•ã‚ãšï¼‰
            try:
                self.logger.info(f"[DEBUG] LLMåˆ¤æ–­ã‚’é–‹å§‹...")
                judgment = await self._judge_and_process_result(
                    tool=tool,
                    current_params=current_params,
                    original_params=original_params,
                    result=raw_result,
                    attempt=attempt + 1,
                    max_retries=max_retries,
                    description=description
                )
                self.logger.info(f"[DEBUG] LLMåˆ¤æ–­å®Œäº†")
                
            except Exception as e:
                self.logger.error(f"[DEBUG] LLMåˆ¤æ–­ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")
                # LLMåˆ¤æ–­ã§ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã€çµæœã‚’ãã®ã¾ã¾è¿”ã™
                return raw_result
            
            # 3. LLMã®åˆ¤æ–­ã«åŸºã¥ã„ã¦è¡Œå‹•
            if judgment.get("needs_retry", False) and attempt < max_retries:
                self.logger.info(f"[ãƒªãƒˆãƒ©ã‚¤] {attempt + 1}/{max_retries}: {judgment.get('error_reason', 'LLMåˆ¤æ–­ã«ã‚ˆã‚‹ãƒªãƒˆãƒ©ã‚¤')}")
                
                # ä¿®æ­£ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å®Ÿè¡Œï¼ˆå…ƒã®paramsã¯ä¿æŒï¼‰
                corrected_params = judgment.get("corrected_params", current_params)
                if corrected_params != current_params:
                    self.logger.info(f"[ä¿®æ­£] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿®æ­£: {safe_str(corrected_params)}")
                    current_params = corrected_params
                
                continue
            
            # æˆåŠŸã¾ãŸã¯æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°åˆ°é”
            return judgment.get("processed_result", raw_result)
        
        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«åˆ°é”
        return judgment.get("processed_result", "æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«åˆ°é”ã—ã¾ã—ãŸã€‚")
    
    async def _judge_and_process_result(
        self, 
        tool: str, 
        current_params: Dict,
        original_params: Dict, 
        result: Any,
        attempt: int = 1,
        max_retries: int = 3,
        description: str = ""
    ) -> Dict[str, Any]:
        """
        LLMã«ã‚ˆã‚‹ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã®åˆ¤æ–­ã¨å‡¦ç†
        
        Args:
            tool: ãƒ„ãƒ¼ãƒ«å
            current_params: ç¾åœ¨å®Ÿè¡Œã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            original_params: å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¿®æ­£ã®åŸºæº–ï¼‰
            result: ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ
            attempt: ç¾åœ¨ã®è©¦è¡Œå›æ•°
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            description: ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã®èª¬æ˜
            
        Returns:
            åˆ¤æ–­çµæœè¾æ›¸
        """
        # çµæœã‚’å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›
        result_str = safe_str(result)
        current_params_str = safe_str(current_params)
        original_params_str = safe_str(original_params)
        
        # ç¾åœ¨ã®ä¼šè©±æ–‡è„ˆã‚’å–å¾—
        current_query = getattr(self, 'current_user_query', 'ï¼ˆä¸æ˜ï¼‰')
        
        prompt = f"""ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’åˆ¤æ–­ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å®Ÿè¡Œçµæœã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

## ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯
ã‚¿ã‚¹ã‚¯: {description or "ã‚¿ã‚¹ã‚¯ã®èª¬æ˜ãªã—"}

## å®Ÿè¡Œæƒ…å ±
- ãƒ„ãƒ¼ãƒ«å: {tool}
- ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {current_params_str}
- å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {original_params_str}
- è©¦è¡Œå›æ•°: {attempt}/{max_retries + 1}
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: {current_query}

## å®Ÿè¡Œçµæœ
{result_str}

## åˆ¤æ–­åŸºæº–
1. **æˆåŠŸåˆ¤å®š**: æœŸå¾…ã•ã‚Œã‚‹çµæœãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹
2. **å¤±æ•—åˆ¤å®š**: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã€å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹
3. **ãƒªãƒˆãƒ©ã‚¤åˆ¤å®š**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿®æ­£ã™ã‚Œã°æˆåŠŸã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹

## **é‡è¦**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£æ™‚ã®ãƒ«ãƒ¼ãƒ«
- **ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã®ç›®çš„ã‚’å¿…ãšå°Šé‡ã—ã¦ãã ã•ã„**
- ä¿®æ­£ã¯å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ{original_params_str}ï¼‰ã‚’åŸºæº–ã«è¡Œã£ã¦ãã ã•ã„
- ä»–ã®ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤‰æ›´ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“
- ä¾‹ï¼šã€ŒBeijingã€ã®å¤©æ°—å–å¾—ãªã‚‰ â†’ ã€ŒBeijing, CNã€ç­‰ã«ä¿®æ­£
- ä¾‹ï¼šã€ŒTokyoã€ã®å¤©æ°—å–å¾—ãªã‚‰ â†’ ã€ŒTokyo, JPã€ç­‰ã«ä¿®æ­£

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
{{
    "is_success": boolean,
    "needs_retry": boolean,
    "error_reason": "ã‚¨ãƒ©ãƒ¼ã®ç†ç”±ï¼ˆå¤±æ•—æ™‚ã®ã¿ï¼‰",
    "corrected_params": {{å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŸºæº–ã¨ã—ãŸä¿®æ­£æ¡ˆ}},
    "processed_result": "ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã®æ•´å½¢æ¸ˆã¿çµæœ",
    "summary": "å®Ÿè¡Œçµæœã®è¦ç´„"
}}

## ä¿®æ­£ä¾‹
- æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ â†’ ã‚³ãƒ¼ãƒ‰ã‚’æ­£ã—ã„æ§‹æ–‡ã«ä¿®æ­£
- éƒ½å¸‚åã‚¨ãƒ©ãƒ¼ â†’ å›½ã‚³ãƒ¼ãƒ‰ä»˜ãã«ä¿®æ­£ï¼ˆä¾‹ï¼šBeijing â†’ Beijing, CNï¼‰
- æ—¥æœ¬èªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â†’ è‹±èªã«å¤‰æ›
- ã‚»ãƒŸã‚³ãƒ­ãƒ³è¨˜æ³• â†’ è¤‡æ•°è¡Œã«åˆ†è§£"""

        try:
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "system", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            raw_response = response.choices[0].message.content
            self.logger.debug(f"[LLMç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹] {safe_str(raw_response)[:500]}")
            
            judgment = json.loads(raw_response)
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆè©³ç´°ç‰ˆï¼‰
            self.logger.info(f"[LLMåˆ¤æ–­] æˆåŠŸ: {judgment.get('is_success')}, ãƒªãƒˆãƒ©ã‚¤å¿…è¦: {judgment.get('needs_retry')}")
            if judgment.get('needs_retry'):
                self.logger.info(f"[LLMç†ç”±] {judgment.get('error_reason', 'ä¸æ˜')}")
                if judgment.get('corrected_params'):
                    self.logger.info(f"[LLMä¿®æ­£æ¡ˆ] {safe_str(judgment.get('corrected_params'))[:200]}")
            else:
                self.logger.info(f"[LLMåˆ¤æ–­ç†ç”±] ãƒªãƒˆãƒ©ã‚¤ä¸è¦ - {judgment.get('summary', 'è©³ç´°ä¸æ˜')}")
            
            return judgment
            
        except Exception as e:
            self.logger.error(f"[LLMåˆ¤æ–­ã‚¨ãƒ©ãƒ¼] {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: çµæœã‚’ãã®ã¾ã¾è¿”ã™
            return {
                "is_success": True,
                "needs_retry": False,
                "processed_result": result_str,
                "summary": "LLMåˆ¤æ–­ã«å¤±æ•—ã—ã¾ã—ãŸã€‚çµæœã‚’ãã®ã¾ã¾è¡¨ç¤ºã—ã¾ã™ã€‚"
            }
    
    
    async def _execute_planned_task(self, task: Dict, step_num: int, total: int, execution_context: List[Dict] = None) -> Dict:
        """è¨ˆç”»ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ"""
        tool = task.get("tool", "")
        params = task.get("params", {})
        description = task.get("description", f"{tool}ã‚’å®Ÿè¡Œ")
        
        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç½®æ›å‡¦ç†
        if execution_context:
            params = self._resolve_placeholders(params, execution_context)
        
        # ã‚¹ãƒ†ãƒƒãƒ—é–‹å§‹ã®è¡¨ç¤º
        self.display.show_step_start(step_num, "?", description)
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œç›´å‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèª
        if self.verbose and tool == "execute_python":
            print(f"[DEBUG] About to execute {tool} with full params:")
            for k, v in params.items():
                print(f"  {k}: {safe_str(v, use_repr=True)}")
        
        self.display.show_tool_call(tool, params)
        
        start_time = time.time()
        
        try:
            # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
            result = await self._execute_tool_with_retry(tool, params, description)
            duration = time.time() - start_time
            
            # ãƒ‡ãƒãƒƒã‚°: å®Ÿè¡Œçµæœã‚’ç¢ºèª
            if self.verbose:
                safe_result = safe_str(result)
                result_preview = safe_result[:200] + "..." if len(safe_result) > 200 else safe_result
                print(f"[DEBUG] Tool: {tool}, Result: {result_preview}")
            
            self.display.show_step_complete(description, duration, success=True)
            
            self.session_stats["successful_tasks"] += 1
            
            return {
                "step": step_num,
                "tool": tool,
                "params": params,
                "result": result,
                "success": True,
                "duration": duration,
                "description": description
            }
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            
            self.session_stats["failed_tasks"] += 1
            
            return {
                "step": step_num,
                "tool": tool,
                "params": params,
                "error": error_msg,
                "success": False,
                "duration": duration,
                "description": description
            }
    
    async def _interpret_planned_results(self, user_query: str, results: List[Dict]) -> str:
        """è¨ˆç”»å®Ÿè¡Œã®çµæœã‚’è§£é‡ˆ"""
        # ç¾åœ¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ã€å‰ã®ã‚¿ã‚¹ã‚¯çµæœã®æ··å…¥ã‚’é˜²ã
        recent_context = self._get_conversation_context_only()
        
        # çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        serializable_results = []
        for r in results:
            result_data = {
                "step": r["step"],
                "tool": r["tool"],
                "success": r["success"],
                "description": r["description"]
            }
            
            if r["success"]:
                # æˆåŠŸæ™‚ã¯çµæœã‚’å«ã‚ã‚‹
                max_length = self.config.get("result_display", {}).get("max_result_length", 1000)
                result_str = str(r["result"])
                
                if len(result_str) <= max_length:
                    result_data["result"] = result_str
                else:
                    # é•·ã™ãã‚‹å ´åˆã¯çœç•¥æƒ…å ±ã‚’è¿½åŠ 
                    result_data["result"] = result_str[:max_length]
                    if self.config.get("result_display", {}).get("show_truncated_info", True):
                        result_data["result"] += f"\n[æ³¨è¨˜: çµæœãŒé•·ã„ãŸã‚{max_length}æ–‡å­—ã§çœç•¥ã€‚å®Ÿéš›ã®çµæœã¯ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™]"
            else:
                result_data["error"] = r["error"]
            
            serializable_results.append(result_data)
        
        # ãƒ‡ãƒãƒƒã‚°: LLMã«æ¸¡ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
        if self.verbose:
            print(f"[DEBUG] Serializable results being sent to LLM:")
            for i, result in enumerate(serializable_results):
                result_preview = str(result.get("result", "N/A"))[:100] + "..." if len(str(result.get("result", "N/A"))) > 100 else str(result.get("result", "N/A"))
                print(f"  [{i+1}] Tool: {result['tool']}, Result: {result_preview}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å–å¾—
        prompt = PromptTemplates.get_result_interpretation_prompt(
            recent_context=recent_context,
            user_query=user_query,
            serializable_results=json.dumps(serializable_results, ensure_ascii=False, indent=2),
            custom_instructions=self.custom_instructions
        )

        try:
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            # æœ€çµ‚å¿œç­”ã‚’å–å¾—
            final_response = response.choices[0].message.content
            
            # Rich UIã®å ´åˆã¯ç¾ã—ãè¡¨ç¤º
            if self.ui_mode == "rich" and hasattr(self.display, 'show_result_panel'):
                # JSONã¾ãŸã¯é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‹ã©ã†ã‹åˆ¤å®š
                if len(final_response) > 100 or final_response.strip().startswith('{'):
                    self.display.show_result_panel("å®Ÿè¡Œçµæœ", final_response, success=True)
                
            # å®Ÿè¡Œçµæœã¨å…±ã«å±¥æ­´ã«ä¿å­˜
            self._add_to_history("assistant", final_response, serializable_results)
            
            return final_response
            
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            successful_results = [r for r in results if r["success"]]
            if successful_results:
                return f"å®Ÿè¡Œå®Œäº†ã—ã¾ã—ãŸã€‚{len(successful_results)}å€‹ã®ã‚¿ã‚¹ã‚¯ãŒæˆåŠŸã—ã¾ã—ãŸã€‚"
            else:
                return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _get_recent_context(self, max_items: int = None) -> str:
        """æœ€è¿‘ã®ä¼šè©±æ–‡è„ˆã‚’å–å¾—ï¼ˆå®Ÿè¡Œçµæœã‚‚å«ã‚€ï¼‰"""
        if max_items is None:
            max_items = self.config["conversation"]["context_limit"]
        
        if not self.conversation_history:
            return ""
        
        recent = self.conversation_history[-max_items:]
        lines = []
        for h in recent:
            role = "User" if h['role'] == "user" else "Assistant"
            # é•·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çœç•¥
            msg = h['message'][:150] + "..." if len(h['message']) > 150 else h['message']
            lines.append(f"{role}: {msg}")
            
            # å®Ÿè¡ŒçµæœãŒã‚ã‚Œã°è¿½åŠ 
            if h.get('execution_results'):
                lines.append(f"å®Ÿè¡Œçµæœãƒ‡ãƒ¼ã‚¿: {self._summarize_results(h['execution_results'])}")
        
        return "\n".join(lines)
    
    def _get_conversation_context_only(self, max_items: int = 3) -> str:
        """
        ä¼šè©±æ–‡è„ˆã®ã¿ã‚’å–å¾—ï¼ˆå®Ÿè¡Œçµæœã‚’é™¤å¤–ï¼‰
        çµæœè§£é‡ˆæ™‚ã«å‰ã®ã‚¿ã‚¹ã‚¯çµæœã®æ··å…¥ã‚’é˜²ã
        """
        if not self.conversation_history:
            return ""
        
        recent = self.conversation_history[-max_items:]
        lines = []
        for h in recent:
            role = "User" if h['role'] == "user" else "Assistant"
            # é•·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çœç•¥
            msg = h['message'][:150] + "..." if len(h['message']) > 150 else h['message']
            lines.append(f"{role}: {msg}")
            # å®Ÿè¡Œçµæœã¯å«ã‚ãªã„ï¼ˆæ··å…¥ã‚’é˜²ããŸã‚ï¼‰
        
        return "\n".join(lines)
    
    def _summarize_results(self, results: List[Dict]) -> str:
        """å®Ÿè¡Œçµæœã‚’è¦ç´„ã—ã¦è¡¨ç¤º"""
        summary_parts = []
        for r in results:
            tool = r.get('tool', 'Unknown')
            success = r.get('success', False)
            
            if success and r.get('result'):
                result_str = str(r['result'])
                # çµæœãŒé•·ã„å ´åˆã¯çŸ­ç¸®
                if len(result_str) > 100:
                    result_str = result_str[:97] + "..."
                summary_parts.append(f"{tool}: {result_str}")
            else:
                summary_parts.append(f"{tool}: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        return " | ".join(summary_parts[:3])  # æœ€å¤§3ã¤ã®çµæœã‚’è¡¨ç¤º
    
    def _add_to_history(self, role: str, message: str, execution_results: List[Dict] = None):
        """ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆå®Ÿè¡Œçµæœã‚‚å«ã‚€ï¼‰"""
        history_item = {
            "timestamp": datetime.now().isoformat(),
            "role": role,
            "message": message
        }
        
        # å®Ÿè¡ŒçµæœãŒã‚ã‚Œã°è¿½åŠ 
        if execution_results:
            history_item["execution_results"] = execution_results
        
        self.conversation_history.append(history_item)
        
        # å±¥æ­´ã®é•·ã•åˆ¶é™
        max_history = self.config["conversation"]["max_history"]
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]
    
    def _show_execution_metrics(self):
        """å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º"""
        if not self.config.get("development", {}).get("show_statistics", True):
            return
            
        print("\n" + "=" * 50)
        print("ğŸ“Š å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        print("=" * 50)
        
        # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆçµ±è¨ˆ
        total_attempts = (self.execution_metrics["task_generation_success"] + 
                         self.execution_metrics["task_generation_total_failures"])
        if total_attempts > 0:
            success_rate = (self.execution_metrics["task_generation_success"] / total_attempts) * 100
            print(f"ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”ŸæˆæˆåŠŸç‡: {success_rate:.1f}% ({self.execution_metrics['task_generation_success']}/{total_attempts})")
        
        if self.execution_metrics["task_generation_retry_success"] > 0:
            print(f"ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ: {self.execution_metrics['task_generation_retry_success']}å›")
        
        if self.execution_metrics["json_parse_errors"] > 0:
            print(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {self.execution_metrics['json_parse_errors']}å›")
            
        if self.execution_metrics["timeout_count"] > 0:
            print(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ: {self.execution_metrics['timeout_count']}å›")
            
        if self.execution_metrics["fallback_usage"] > 0:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {self.execution_metrics['fallback_usage']}å›")
        
        if self.execution_metrics["total_task_lists"] > 0:
            avg_tasks = self.execution_metrics["average_task_count"] / self.execution_metrics["total_task_lists"]
            print(f"å¹³å‡ã‚¿ã‚¹ã‚¯æ•°: {avg_tasks:.1f}å€‹")
        
        print("=" * 50)
    
    def _show_session_statistics(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’è¡¨ç¤º"""
        total_time = (datetime.now() - self.session_stats["start_time"]).total_seconds()
        
        self.display.show_result_summary(
            total_tasks=self.session_stats["successful_tasks"] + self.session_stats["failed_tasks"],
            successful=self.session_stats["successful_tasks"],
            failed=self.session_stats["failed_tasks"],
            total_duration=total_time
        )
        
        if self.config["development"]["show_api_calls"]:
            print(f"APIå‘¼ã³å‡ºã—å›æ•°: {self.session_stats['total_api_calls']}")
    
    async def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾"""
        # çµ‚äº†æ™‚ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        self._show_execution_metrics()
        await self.connection_manager.close()


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    agent = MCPAgentV4()
    await agent.initialize()
    
    try:
        print("\nMCP Agent V4 ãŒæº–å‚™å®Œäº†ã—ã¾ã—ãŸï¼")
        print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("-" * 60)
        
        while True:
            try:
                if hasattr(agent.display, 'input_prompt') and agent.ui_mode == "rich":
                    user_input = agent.display.input_prompt("Agent").strip()
                else:
                    user_input = input("\nAgent> ").strip()
            except (EOFError, KeyboardInterrupt):
                break
            
            if user_input.lower() in ['quit', 'exit', 'çµ‚äº†']:
                break
            
            if not user_input:
                continue
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
            response = await agent.process_request(user_input)
            
            # Rich UIã®å ´åˆã¯Markdownæ•´å½¢è¡¨ç¤º
            if agent.ui_mode == "rich" and hasattr(agent.display, 'show_markdown_result'):
                agent.display.show_markdown_result(response)
            else:
                print(f"\n{response}")
    
    except KeyboardInterrupt:
        print("\n\n[ä¸­æ–­] Ctrl+CãŒæŠ¼ã•ã‚Œã¾ã—ãŸã€‚")
    finally:
        await agent.close()
        print("\nMCP Agent V4 ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    asyncio.run(main())