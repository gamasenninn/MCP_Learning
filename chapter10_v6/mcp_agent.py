#!/usr/bin/env python3
"""
MCP Agent - Interactive Dialogue Engine
Claude Codeé¢¨ã®å¯¾è©±å‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

ä¸»ãªç‰¹å¾´ï¼š
- å¯¾è©±çš„é€æ¬¡å®Ÿè¡Œï¼ˆä¾å­˜é–¢ä¿‚ã®è‡ªå‹•è§£æ±ºï¼‰
- ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ä»˜ãã‚¿ã‚¹ã‚¯è¡¨ç¤º
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹
- éå»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®çŸ¥è¦‹ã‚’æ´»ã‹ã—ãŸè¨­è¨ˆ
"""

import os
import json
import asyncio
import time
import yaml
from typing import Dict, List, Any, Optional
from datetime import datetime
from openai import AsyncOpenAI

from connection_manager import ConnectionManager
from display_manager import DisplayManager
from error_handler import ErrorHandler
from prompts import PromptTemplates
from utils import Logger, safe_str
from state_manager import StateManager, TaskState
from task_manager import TaskManager

# Rich UI support
try:
    from display_manager_rich import RichDisplayManager
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False




class MCPAgent:
    """
    Claude Codeé¢¨ã®å¯¾è©±å‹MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    éå»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰å¼•ãç¶™ã„ã è¦ç´ :
    - AGENT.mdã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    - ä¼šè©±æ–‡è„ˆã®æ´»ç”¨
    - NO_TOOLåˆ¤å®š
    
    ç¾åœ¨ã®ä¸»è¦æ©Ÿèƒ½:
    - å¯¾è©±çš„é€æ¬¡å®Ÿè¡Œ
    - ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®å¯è¦–åŒ–
    - ä¾å­˜é–¢ä¿‚ã®è‡ªå‹•è§£æ±º
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """åˆæœŸåŒ–"""
        self.config = self._load_config(config_path)
        
        # UI ãƒ¢ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦é©åˆ‡ãªDisplayManagerã‚’é¸æŠ
        ui_mode = self.config.get("display", {}).get("ui_mode", "basic")
        
        if ui_mode == "rich" and RICH_AVAILABLE:
            self.display = RichDisplayManager(
                show_timing=self.config["display"]["show_timing"],
                show_thinking=self.config["display"]["show_thinking"]
            )
            self.ui_mode = "rich"
        else:
            if ui_mode == "rich" and not RICH_AVAILABLE:
                print("[WARNING] Rich UI requested but rich library not available. Using basic UI.")
            self.display = DisplayManager(
                show_timing=self.config["display"]["show_timing"],
                show_thinking=self.config["display"]["show_thinking"]
            )
            self.ui_mode = "basic"
        
        # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.llm = AsyncOpenAI()
        
        # MCPæ¥ç¶šç®¡ç†ï¼ˆV3ã‹ã‚‰æµç”¨ï¼‰
        self.connection_manager = ConnectionManager()
        
        # ã‚¨ãƒ©ãƒ¼å‡¦ç†å¸ä»¤å¡”ï¼ˆV4æ–°æ©Ÿèƒ½ï¼‰
        self.error_handler = ErrorHandler(
            config=self.config,
            llm=self.llm,
            verbose=self.config.get("development", {}).get("verbose", True)
        )
        
        # çŠ¶æ…‹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆV6æ–°æ©Ÿèƒ½ï¼‰
        self.state_manager = StateManager()
        self.task_manager = TaskManager(self.state_manager, self.llm)
        
        # ä¼šè©±å±¥æ­´ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
        self.conversation_history: List[Dict] = []
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
        self.session_stats = {
            "start_time": datetime.now(),
            "total_requests": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_api_calls": 0
        }
        
        # å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        self.execution_metrics = {
            "task_generation_success": 0,
            "task_generation_failures": 0,
            "task_generation_retry_success": 0,
            "task_generation_total_failures": 0,
            "json_parse_errors": 0,
            "timeout_count": 0,
            "fallback_usage": 0,
            "average_task_count": 0.0,
            "total_task_lists": 0
        }
        
        # AGENT.mdèª­ã¿è¾¼ã¿ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
        self.custom_instructions = self._load_agent_md()
        
        # Loggerã‚’åˆæœŸåŒ–
        self.verbose = self.config.get("development", {}).get("verbose", True)
        self.logger = Logger(verbose=self.verbose)
        
        if self.verbose:
            self.display.show_banner()
            if self.ui_mode == "rich":
                self.logger.info("Rich UI mode enabled")
            else:
                self.logger.info("Basic UI mode enabled")
    
    def _load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆå¿…é ˆï¼‰"""
        if not os.path.exists(config_path):
            raise FileNotFoundError(
                f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                f"'config.sample.yaml' ã‚’ '{config_path}' ã«ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚"
            )
        
        with open(config_path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    
    def _load_agent_md(self) -> str:
        """AGENT.mdã‚’èª­ã¿è¾¼ã¿ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰"""
        agent_md_path = "AGENT.md"
        
        if os.path.exists(agent_md_path):
            try:
                with open(agent_md_path, "r", encoding="utf-8") as f:
                    content = f.read()
                if hasattr(self, 'logger'):
                    self.logger.config_info(f"AGENT.mdã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(content)}æ–‡å­—)")
                elif self.config.get("development", {}).get("verbose", False):
                    print(f"[è¨­å®š] AGENT.mdã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(content)}æ–‡å­—)")
                return content
            except Exception as e:
                print(f"[è­¦å‘Š] AGENT.mdèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return ""
        else:
            if self.config.get("development", {}).get("verbose", False):
                print("[æƒ…å ±] AGENT.mdãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆåŸºæœ¬èƒ½åŠ›ã®ã¿ã§å‹•ä½œï¼‰")
            return ""
    
    async def initialize(self, session_id: Optional[str] = None):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–"""
        if self.verbose:
            print(f"\n[æŒ‡ç¤ºæ›¸] {'ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤ºã‚ã‚Š' if self.custom_instructions else 'åŸºæœ¬èƒ½åŠ›ã®ã¿'}")
            print("=" * 60)
        
        # MCPæ¥ç¶šç®¡ç†ã‚’åˆæœŸåŒ–ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
        await self.connection_manager.initialize()
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–ï¼ˆV6æ–°æ©Ÿèƒ½ï¼‰
        actual_session_id = await self.state_manager.initialize_session(session_id)
        
        if self.verbose:
            print(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³] {actual_session_id}")
            
            # å¾©å…ƒã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã¯é€šçŸ¥
            if self.state_manager.has_pending_tasks():
                pending_count = len(self.state_manager.get_pending_tasks())
                print(f"[å¾©å…ƒ] æœªå®Œäº†ã‚¿ã‚¹ã‚¯ãŒ{pending_count}å€‹ã‚ã‚Šã¾ã™")
        
        return actual_session_id
    
    async def process_request(self, user_query: str) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å¯¾è©±çš„ã«å‡¦ç†ï¼ˆæ ¸å¿ƒæ©Ÿèƒ½ï¼‰
        
        ç‰¹å¾´:
        - ä¸€åº¦ã«å…¨ã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£ã›ãšã€ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å¯¾è©±
        - å‰ã®çµæœã‚’è¦‹ã¦ã‹ã‚‰æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š
        - å®Ÿè¡Œéç¨‹ã‚’è¦–è¦šçš„ã«è¡¨ç¤º
        """
        self.session_stats["total_requests"] += 1
        
        if self.verbose:
            print(f"\n[ãƒªã‚¯ã‚¨ã‚¹ãƒˆ #{self.session_stats['total_requests']}] {user_query}")
            print("-" * 60)
        
        # ä¼šè©±æ–‡è„ˆã‚’è¡¨ç¤º
        context_count = min(len(self.conversation_history), 
                          self.config["conversation"]["context_limit"])
        if context_count > 0:
            self.display.show_context_info(context_count)
        
        try:
            # å¯¾è©±çš„å®Ÿè¡Œã®é–‹å§‹
            response = await self._execute_interactive_dialogue(user_query)
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆV3ã‹ã‚‰ç¶™æ‰¿ï¼‰
            # å®Ÿè¡Œçµæœã«ã¤ã„ã¦ã¯å„å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ã§è¿½åŠ ã•ã‚Œã‚‹
            self._add_to_history("user", user_query)
            
            return response
            
        except Exception as e:
            error_msg = f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            if self.verbose:
                print(f"[ã‚¨ãƒ©ãƒ¼] {error_msg}")
            return error_msg
    
    async def _execute_interactive_dialogue(self, user_query: str) -> str:
        """
        V6çµ±åˆå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ - çŠ¶æ…‹ç®¡ç†ã¨CLARIFICATIONå¯¾å¿œ
        
        æ–°æ©Ÿèƒ½:
        - çŠ¶æ…‹ã®æ°¸ç¶šåŒ–
        - CLARIFICATIONã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª
        - ã‚¿ã‚¹ã‚¯ã®ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½
        """
        # ç¾åœ¨ã®ã‚¯ã‚¨ãƒªã‚’ä¿å­˜ï¼ˆLLMåˆ¤æ–­ã§ä½¿ç”¨ï¼‰
        self.current_user_query = user_query
        
        # çŠ¶æ…‹ã«ä¼šè©±ã‚’è¨˜éŒ²
        await self.state_manager.add_conversation_entry("user", user_query)
        
        # æœªå®Œäº†ã®ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
        if self.state_manager.has_pending_tasks():
            return await self._handle_pending_tasks(user_query)
        
        self.display.show_analysis("ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æä¸­...")
        
        # ã¾ãšå‡¦ç†æ–¹å¼ã‚’åˆ¤å®šï¼ˆCLARIFICATIONå¯¾å¿œç‰ˆï¼‰
        execution_result = await self._determine_execution_type_v6(user_query)
        execution_type = execution_result.get("type", "SIMPLE")
        
        # çŠ¶æ…‹ã«å®Ÿè¡Œã‚¿ã‚¤ãƒ—ã‚’è¨˜éŒ²
        await self.state_manager.set_user_query(user_query, execution_type)
        
        if execution_type == "NO_TOOL":
            response = execution_result.get("response", "äº†è§£ã—ã¾ã—ãŸã€‚")
            await self.state_manager.add_conversation_entry("assistant", response)
            self._add_to_history("assistant", response)
            return response
        elif execution_type == "CLARIFICATION":
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ç¢ºèªãŒå¿…è¦
            return await self._handle_clarification_needed(user_query, execution_result)
        else:
            # SIMPLE/COMPLEXçµ±åˆï¼šå…¨ã¦ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œè¦æ±‚ã‚’çµ±ä¸€ãƒ¡ã‚½ãƒƒãƒ‰ã§å‡¦ç†
            return await self._execute_with_tasklist_v6(user_query)
    
    async def _determine_execution_type_v6(self, user_query: str) -> Dict:
        """V6ç‰ˆ: CLARIFICATIONå¯¾å¿œã®å®Ÿè¡Œæ–¹å¼åˆ¤å®š"""
        recent_context = self._get_recent_context()
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
        tools_info = self.connection_manager.format_tools_for_llm()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å–å¾—ï¼ˆV6å¯¾å¿œç‰ˆï¼‰
        prompt = PromptTemplates.get_execution_type_determination_prompt_v6(
            recent_context=recent_context,
            user_query=user_query,
            tools_info=tools_info
        )

        try:
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "system", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            content = safe_str(response.choices[0].message.content)
            result = json.loads(content)
            
            # V6ç‰ˆã§ã¯ CLARIFICATION ã‚‚å«ã‚€
            # SIMPLE/COMPLEXçµ±åˆã®ãŸã‚ã€NO_TOOL, CLARIFICATIONä»¥å¤–ã¯å…¨ã¦TOOLã«çµ±ä¸€
            if result.get('type') in ['SIMPLE', 'COMPLEX']:
                result['type'] = 'TOOL'
            
            
            self.logger.info(f"åˆ¤å®š: {result.get('type', 'UNKNOWN')} - {result.get('reason', '')}")
            
            return result
            
        except Exception as e:
            print(f"[ã‚¨ãƒ©ãƒ¼] å®Ÿè¡Œæ–¹å¼åˆ¤å®šå¤±æ•—: {e}")
            return {"type": "TOOL", "reason": "åˆ¤å®šã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ"}
    
    async def _handle_pending_tasks(self, user_query: str) -> str:
        """æœªå®Œäº†ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã®å‡¦ç†"""
        pending_tasks = self.state_manager.get_pending_tasks()
        
        # CLARIFICATIONã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆ
        if self.task_manager.has_clarification_tasks():
            for task in pending_tasks:
                if task.tool == "CLARIFICATION" and task.status == "pending":
                    # CLARIFICATIONã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã¨ã—ã¦ãƒãƒ¼ã‚¯
                    await self.state_manager.move_task_to_completed(task.task_id, {"user_response": user_query})
                    
                    # å…ƒã®ã‚¯ã‚¨ãƒªã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿œç­”ã‚’çµ„ã¿åˆã‚ã›ã¦æ–°ã—ã„ã‚¯ã‚¨ãƒªã‚’ä½œæˆ
                    original_query = task.params.get("user_query", "")
                    question = task.params.get("question", "")
                    
                    # ã‚ˆã‚Šæ˜ç¢ºãªå½¢å¼ã§LLMãŒç†è§£ã—ã‚„ã™ãæ§‹æˆ
                    combined_query = f"{original_query}ã€‚{user_query}ã€‚"
                    
                    # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦æ–°ã—ã„ã‚¯ã‚¨ãƒªã¨ã—ã¦å‡¦ç†
                    await self.state_manager.set_user_query(combined_query, "TOOL")
                    
                    # LLMã«æ–°ã—ã„ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã•ã›ã‚‹
                    return await self._execute_with_tasklist_v6(combined_query)
        
        # é€šå¸¸ã®ã‚¿ã‚¹ã‚¯ã‚’ç¶™ç¶šå®Ÿè¡Œ
        return await self._continue_pending_tasks(user_query)
    
    async def _handle_clarification_needed(self, user_query: str, execution_result: Dict) -> str:
        """CLARIFICATIONå¿…è¦æ™‚ã®å‡¦ç†"""
        clarification_info = execution_result.get('clarification', {})
        
        # CLARIFICATIONã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
        clarification_task = TaskState(
            task_id=f"clarification_{int(time.time())}",
            tool="CLARIFICATION",
            params={
                "question": clarification_info.get('question', 'è©³ç´°æƒ…å ±ã‚’ãŠæ•™ãˆãã ã•ã„'),
                "context": f"è¦æ±‚: {user_query}",
                "user_query": user_query
            },
            description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèª",
            status="pending"
        )
        
        await self.state_manager.add_pending_task(clarification_task)
        
        # CLARIFICATIONã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        return await self.task_manager.execute_clarification_task(clarification_task)
    
    async def _handle_clarification_task(self, task: TaskState) -> str:
        """CLARIFICATIONã‚¿ã‚¹ã‚¯ã®å‡¦ç†"""
        return await self.task_manager.execute_clarification_task(task)
    
    async def _continue_pending_tasks(self, user_query: str) -> str:
        """ä¿ç•™ä¸­ã‚¿ã‚¹ã‚¯ã®ç¶™ç¶šå®Ÿè¡Œ"""
        next_task = self.task_manager.get_next_executable_task()
        
        if not next_task:
            return "å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        # ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        return await self._execute_single_task_v6(next_task)
    
    async def _execute_single_task_v6(self, task: TaskState) -> str:
        """å˜ä¸€ã‚¿ã‚¹ã‚¯ã®V6å®Ÿè¡Œ"""
        try:
            # ä¾å­˜é–¢ä¿‚ã‚’è§£æ±º
            completed_tasks = self.state_manager.get_completed_tasks()
            resolved_task = await self.task_manager.resolve_task_dependencies(task, completed_tasks)
            
            # ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
            result = await self.connection_manager.call_tool(resolved_task.tool, resolved_task.params)
            
            # çµæœã‚’å®‰å…¨ãªå½¢å¼ã«å¤‰æ›
            safe_result = safe_str(result)
            
            # çµæœã‚’çŠ¶æ…‹ã«ä¿å­˜
            await self.state_manager.move_task_to_completed(task.task_id, safe_result)
            
            return f"ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸ: {task.description}\nçµæœ: {safe_result}"
            
        except Exception as e:
            await self.state_manager.move_task_to_completed(task.task_id, error=str(e))
            return f"ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {task.description}\nã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    async def _execute_with_tasklist_v6(self, user_query: str) -> str:
        """V6ç‰ˆã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ - çŠ¶æ…‹ç®¡ç†å¯¾å¿œ"""
        
        # V6ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ç”Ÿæˆ
        task_list_spec = await self._generate_simple_task_list_v6(user_query)
        
        if not task_list_spec:
            error_msg = (f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚{user_query}ã®å‡¦ç†æ–¹æ³•ã‚’æ±ºå®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                       f"åˆ¥ã®è¡¨ç¾ã§å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            return error_msg
        
        # TaskStateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆï¼ˆCLARIFICATIONå‡¦ç†ã‚’å«ã‚€ï¼‰
        tasks = await self.task_manager.create_tasks_from_list(task_list_spec, user_query)
        
        # ã‚¿ã‚¹ã‚¯ã‚’çŠ¶æ…‹ç®¡ç†ã«è¿½åŠ 
        for task in tasks:
            await self.state_manager.add_pending_task(task)
        
        # CLARIFICATIONã‚¿ã‚¹ã‚¯ãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆã¯å„ªå…ˆå‡¦ç†
        if self.task_manager.has_clarification_tasks():
            clarification_task = None
            for task in tasks:
                if task.tool == "CLARIFICATION":
                    clarification_task = task
                    break
            
            if clarification_task:
                return await self._handle_clarification_task(clarification_task)
        
        # é€šå¸¸ã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆå®Ÿè¡Œ
        return await self._execute_task_sequence_v6(tasks, user_query)
    
    async def _execute_task_sequence_v6(self, tasks: List[TaskState], user_query: str) -> str:
        """V6ç‰ˆã‚¿ã‚¹ã‚¯é †æ¬¡å®Ÿè¡Œ"""
        
        # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºï¼ˆæ—¢å­˜ã®è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰
        task_list_for_display = [
            {
                "tool": task.tool,
                "description": task.description,
                "params": task.params
            }
            for task in tasks if task.tool != "CLARIFICATION"
        ]
        
        if task_list_for_display:
            # V6ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤º
            print("\n[ã‚¿ã‚¹ã‚¯ä¸€è¦§]")
            for i, task in enumerate(task_list_for_display):
                print(f"  [ ] {task['description']}")
        
        # å®Ÿè¡Œçµæœã‚’è¿½è·¡
        completed = []
        failed = []
        execution_context = []
        
        # ã‚¿ã‚¹ã‚¯ã‚’é †æ¬¡å®Ÿè¡Œ
        executable_tasks = [t for t in tasks if t.tool != "CLARIFICATION"]
        
        for i, task in enumerate(executable_tasks):
            # V6ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«é€²è¡ŒçŠ¶æ³è¡¨ç¤º
            print(f"\n[å®Ÿè¡Œä¸­] {task.description}...")
            
            # LLMãƒ™ãƒ¼ã‚¹ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è§£æ±º
            resolved_params = await self._generate_params_with_llm_v6(task, execution_context)
            
            # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
            start_time = time.time()
            result = await self._execute_tool_with_retry(
                tool=task.tool,
                params=resolved_params,
                description=task.description
            )
            duration = time.time() - start_time
            
            # çµæœã‚’å®‰å…¨ãªå½¢å¼ã«å¤‰æ›
            safe_result = safe_str(result)
            
            # æˆåŠŸæ™‚ã®å‡¦ç†
            await self.state_manager.move_task_to_completed(task.task_id, safe_result)
            completed.append(i)
            print(f"[å®Œäº†] {task.description}")
            
            execution_context.append({
                "success": True,
                "result": safe_result,
                "duration": duration,
                "task_description": task.description
            })
        
        # V6ç”¨ã®æœ€çµ‚çŠ¶æ³è¡¨ç¤º
        if completed:
            print(f"\n[å®Œäº†] {len(completed)}å€‹ã®ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸å®Œäº†")
        if failed:
            print(f"[å¤±æ•—] {len(failed)}å€‹ã®ã‚¿ã‚¹ã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ")
        
        # çµæœã‚’LLMã§è§£é‡ˆ
        return await self._interpret_planned_results(user_query, execution_context)
    
    def _format_context_for_llm(self, execution_context: List[Dict]) -> str:
        """å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’LLMç”¨ã«æ•´å½¢"""
        if not execution_context:
            return "ï¼ˆã¾ã å®Ÿè¡Œçµæœãªã—ï¼‰"
        
        formatted_context = []
        for i, ctx in enumerate(execution_context, 1):
            tool = ctx.get("tool", "ä¸æ˜")
            result = safe_str(ctx.get("result", ""))[:200]  # æœ€åˆã®200æ–‡å­—
            description = ctx.get("description", f"{tool}ã‚’å®Ÿè¡Œ")
            
            formatted_context.append(f"ã‚¹ãƒ†ãƒƒãƒ—{i}: {description}")
            formatted_context.append(f"  ãƒ„ãƒ¼ãƒ«: {tool}")
            formatted_context.append(f"  çµæœ: {result}")
            if len(safe_str(ctx.get("result", ""))) > 200:
                formatted_context.append("  ï¼ˆçµæœãŒé•·ã„ãŸã‚çœç•¥...ï¼‰")
        
        return "\n".join(formatted_context)
    
    async def _generate_params_with_llm_v6(self, task: TaskState, execution_context: List[Dict]) -> Dict:
        """V6ç”¨: LLMãŒå®Ÿè¡Œæ–‡è„ˆã‚’ç†è§£ã—ã¦ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        tool = task.tool
        params = task.params
        description = task.description
        
        # å®Ÿè¡Œæ–‡è„ˆã‹ã‚‰çµæœæƒ…å ±ã‚’æŠ½å‡º
        context_info = []
        if execution_context:
            for i, ctx in enumerate(execution_context):
                if ctx.get("success"):
                    result_str = str(ctx.get("result", ""))
                    task_desc = ctx.get("task_description", "ä¸æ˜ãªã‚¿ã‚¹ã‚¯")
                    context_info.append(f"ã‚¿ã‚¹ã‚¯{i+1}: {task_desc} â†’ çµæœ: {result_str}")
        
        context_str = "\n".join(context_info) if context_info else "å‰ã®å®Ÿè¡Œçµæœã¯ã‚ã‚Šã¾ã›ã‚“"
        
        prompt = f"""æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€å®Ÿè¡Œå±¥æ­´ã‹ã‚‰é©åˆ‡ã«æ±ºå®šã—ã¦ãã ã•ã„ã€‚

## å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯
- ãƒ„ãƒ¼ãƒ«: {tool}
- èª¬æ˜: {description}
- å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {json.dumps(params, ensure_ascii=False)}

## ã“ã‚Œã¾ã§ã®å®Ÿè¡Œå±¥æ­´
{context_str}

## æŒ‡ç¤º
å‰ã®å®Ÿè¡Œçµæœã‚’å‚è€ƒã«ã—ã¦ã€ã“ã®ã‚¿ã‚¹ã‚¯ã«æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚
å‰ã®ã‚¿ã‚¹ã‚¯ã®æ•°å€¤çµæœã‚’ä½¿ã†å ´åˆã¯ã€ãã®æ•°å€¤ã‚’ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
```json
{{
  "resolved_params": {{å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤}},
  "reasoning": "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºå®šã—ãŸç†ç”±"
}}
```"""

        try:
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            response_text = response.choices[0].message.content
            
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
                resolved_params = result.get("resolved_params", params)
                reasoning = result.get("reasoning", "")
                
                if self.verbose:
                    print(f"[V6ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£æ±º] {reasoning}")
                
                return resolved_params
            
            if self.verbose:
                print(f"[V6ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£æ±ºå¤±æ•—] JSONè§£æã‚¨ãƒ©ãƒ¼: {response_text[:100]}...")
            return params
            
        except Exception as e:
            if self.verbose:
                print(f"[V6ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£æ±ºã‚¨ãƒ©ãƒ¼] {e}")
            return params
    
    def _build_judgment_prompt(
        self, 
        tool: str, 
        current_params: Dict,
        original_params: Dict,
        result: Any,
        attempt: int,
        max_retries: int,
        description: str
    ) -> str:
        """LLMåˆ¤æ–­ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ"""
        # çµæœã‚’å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›
        result_str = safe_str(result)
        current_params_str = safe_str(current_params)
        original_params_str = safe_str(original_params)
        
        # ç¾åœ¨ã®ä¼šè©±æ–‡è„ˆã‚’å–å¾—
        current_query = getattr(self, 'current_user_query', 'ï¼ˆä¸æ˜ï¼‰')
        
        return f"""ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’åˆ¤æ–­ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å®Ÿè¡Œçµæœã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

## ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯
ã‚¿ã‚¹ã‚¯: {description or "ã‚¿ã‚¹ã‚¯ã®èª¬æ˜ãªã—"}

## å®Ÿè¡Œæƒ…å ±
- ãƒ„ãƒ¼ãƒ«å: {tool}
- ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {current_params_str}
- å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {original_params_str}
- è©¦è¡Œå›æ•°: {attempt}/{max_retries + 1}
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: {current_query}

## å®Ÿè¡Œçµæœ
{result_str}

## åˆ¤æ–­åŸºæº–
1. **æˆåŠŸåˆ¤å®š**: æœŸå¾…ã•ã‚Œã‚‹çµæœãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹
2. **å¤±æ•—åˆ¤å®š**: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã€å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹
3. **ãƒªãƒˆãƒ©ã‚¤åˆ¤å®š**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿®æ­£ã™ã‚Œã°æˆåŠŸã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹

## **é‡è¦**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£æ™‚ã®ãƒ«ãƒ¼ãƒ«
- **ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã®ç›®çš„ã‚’å¿…ãšå°Šé‡ã—ã¦ãã ã•ã„**
- ä¿®æ­£ã¯å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ{original_params_str}ï¼‰ã‚’åŸºæº–ã«è¡Œã£ã¦ãã ã•ã„
- ä»–ã®ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤‰æ›´ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“
- ä¾‹ï¼šéƒ½å¸‚åã«å›½ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆä¾‹ï¼š"Tokyo" â†’ "Tokyo, JP"ï¼‰

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
{{
    "is_success": boolean,
    "needs_retry": boolean,
    "error_reason": "ã‚¨ãƒ©ãƒ¼ã®ç†ç”±ï¼ˆå¤±æ•—æ™‚ã®ã¿ï¼‰",
    "corrected_params": {{å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŸºæº–ã¨ã—ãŸä¿®æ­£æ¡ˆ}},
    "processed_result": "ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã®æ•´å½¢æ¸ˆã¿çµæœ",
    "summary": "å®Ÿè¡Œçµæœã®è¦ç´„"
}}

## ä¿®æ­£ä¾‹
- æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ â†’ ã‚³ãƒ¼ãƒ‰ã‚’æ­£ã—ã„æ§‹æ–‡ã«ä¿®æ­£
- éƒ½å¸‚åã‚¨ãƒ©ãƒ¼ â†’ å›½ã‚³ãƒ¼ãƒ‰ä»˜ãã«ä¿®æ­£
- æ—¥æœ¬èªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â†’ è‹±èªã«å¤‰æ›
- ã‚»ãƒŸã‚³ãƒ­ãƒ³è¨˜æ³• â†’ è¤‡æ•°è¡Œã«åˆ†è§£"""
    
    async def _call_llm_for_judgment(self, prompt: str) -> Dict:
        """LLMã«åˆ¤æ–­ã‚’ä¾é ¼ã—ã¦JSONçµæœã‚’è¿”ã™"""
        try:
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "system", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            raw_response = response.choices[0].message.content
            self.logger.debug(f"[LLMç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹] {safe_str(raw_response)[:500]}")
            
            return json.loads(raw_response)
            
        except Exception as e:
            self.logger.error(f"[LLMåˆ¤æ–­ã‚¨ãƒ©ãƒ¼] {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æˆåŠŸã¨ã—ã¦æ‰±ã†
            return {
                "is_success": True,
                "needs_retry": False,
                "processed_result": "LLMåˆ¤æ–­ã«å¤±æ•—ã—ã¾ã—ãŸã€‚çµæœã‚’ãã®ã¾ã¾è¡¨ç¤ºã—ã¾ã™ã€‚",
                "summary": "LLMåˆ¤æ–­ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"
            }
    
    def _log_judgment_result(self, judgment: Dict):
        """åˆ¤æ–­çµæœã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›"""
        self.logger.info(f"[LLMåˆ¤æ–­] æˆåŠŸ: {judgment.get('is_success')}, ãƒªãƒˆãƒ©ã‚¤å¿…è¦: {judgment.get('needs_retry')}")
        
        if judgment.get('needs_retry'):
            self.logger.info(f"[LLMç†ç”±] {judgment.get('error_reason', 'ä¸æ˜')}")
            if judgment.get('corrected_params'):
                self.logger.info(f"[LLMä¿®æ­£æ¡ˆ] {safe_str(judgment.get('corrected_params'))[:200]}")
        else:
            self.logger.info(f"[LLMåˆ¤æ–­ç†ç”±] ãƒªãƒˆãƒ©ã‚¤ä¸è¦ - {judgment.get('summary', 'è©³ç´°ä¸æ˜')}")
    
    async def _generate_adaptive_task_list(self, user_query: str, temperature: float = 0.1) -> List[Dict]:
        """
        ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã«å¿œã˜ã¦é©å¿œçš„ã«ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            temperature: LLMã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        recent_context = self._get_conversation_context_only()
        tools_info = self.connection_manager.format_tools_for_llm()
        
        # ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤ºã®æœ‰ç„¡ã§è¤‡é›‘ã•ã‚’åˆ¤å®š
        custom_instructions = self.custom_instructions if self.custom_instructions.strip() else None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å–å¾—
        prompt = PromptTemplates.get_adaptive_task_list_prompt(
            recent_context=recent_context,
            user_query=user_query,
            tools_info=tools_info,
            custom_instructions=custom_instructions
        )

        try:
            self.session_stats["total_api_calls"] += 1
            
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "system", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=temperature
            )
            
            content = safe_str(response.choices[0].message.content)
            result = json.loads(content)
            tasks = result.get("tasks", [])
            
            # ã‚¿ã‚¹ã‚¯æ•°ã«å¿œã˜ã¦ãƒ­ã‚°å‡ºåŠ›
            task_type_label = "è©³ç´°" if custom_instructions else "ã‚·ãƒ³ãƒ—ãƒ«"
            self.logger.info(f"{task_type_label}ã‚¿ã‚¹ã‚¯: {len(tasks)}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ")
            for i, task in enumerate(tasks, 1):
                self.logger.debug(f"  [{i}] Tool: {task.get('tool')}, Params: {safe_str(task.get('params', {}))[:100]}...")
                self.logger.debug(f"      Description: {task.get('description', 'N/A')}")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªå ´åˆã¯æœ€å¤§3ã‚¿ã‚¹ã‚¯ã«åˆ¶é™
            if not custom_instructions and len(tasks) > 3:
                self.logger.info(f"ã‚¿ã‚¹ã‚¯æ•°åˆ¶é™: {len(tasks)} â†’ 3ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãƒ¢ãƒ¼ãƒ‰ï¼‰")
                tasks = tasks[:3]
            
            return tasks
            
        except Exception as e:
            print(f"[ã‚¨ãƒ©ãƒ¼] é©å¿œçš„ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆå¤±æ•—: {e}")
            return []
    
    async def _generate_task_list_with_retry(self, user_query: str) -> List[Dict]:
        """
        ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãé©å¿œçš„ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        retry_config = self.config.get("execution", {}).get("retry_strategy", {})
        max_retries = retry_config.get("max_retries", 3)
        use_progressive = retry_config.get("progressive_temperature", True)
        initial_temp = retry_config.get("initial_temperature", 0.1)
        temp_increment = retry_config.get("temperature_increment", 0.2)
        
        last_error = None
        
        for attempt in range(max_retries):
            try:
                # ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–temperatureèª¿æ•´
                if use_progressive and attempt > 0:
                    temperature = min(initial_temp + (attempt * temp_increment), 0.9)
                else:
                    temperature = initial_temp
                
                # çµ±ä¸€ã•ã‚ŒãŸé©å¿œçš„ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆã‚’ä½¿ç”¨
                task_list = await self._generate_adaptive_task_list(user_query, temperature)
                
                if task_list:
                    # æˆåŠŸæ™‚ã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
                    if hasattr(self, 'execution_metrics'):
                        self.execution_metrics['task_generation_success'] += 1
                        if attempt > 0:
                            self.execution_metrics['task_generation_retry_success'] += 1
                    
                    if attempt > 0:
                        self.logger.info(f"[æˆåŠŸ] ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ - {attempt + 1}å›ç›®ã®è©¦è¡Œã§æˆåŠŸ")
                    
                    # ã‚¿ã‚¹ã‚¯æ•°åˆ¶é™ï¼ˆå…¨ä½“çš„ãªä¸Šé™ï¼‰
                    max_tasks = self.config.get("execution", {}).get("max_tasks", 10)
                    if len(task_list) > max_tasks:
                        self.logger.warning(f"ã‚¿ã‚¹ã‚¯æ•°åˆ¶é™: {len(task_list)} â†’ {max_tasks}")
                        task_list = task_list[:max_tasks]
                    
                    return task_list
                else:
                    last_error = f"è©¦è¡Œ{attempt + 1}: ç©ºã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ"
                    
            except json.JSONDecodeError as e:
                last_error = f"è©¦è¡Œ{attempt + 1}: JSONè§£æã‚¨ãƒ©ãƒ¼ - {str(e)}"
                self.logger.info(f"[ãƒªãƒˆãƒ©ã‚¤] {last_error}")
            except Exception as e:
                last_error = f"è©¦è¡Œ{attempt + 1}: {str(e)}"
                self.logger.info(f"[ãƒªãƒˆãƒ©ã‚¤] {last_error}")
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            if hasattr(self, 'execution_metrics'):
                self.execution_metrics['task_generation_failures'] += 1
        
        # å…¨ã¦ã®è©¦è¡ŒãŒå¤±æ•—
        self.logger.error(f"[å¤±æ•—] ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ - {max_retries}å›ã®è©¦è¡Œå…¨ã¦ãŒå¤±æ•—")
        self.logger.error(f"æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {last_error}")
        
        if hasattr(self, 'execution_metrics'):
            self.execution_metrics['task_generation_total_failures'] += 1
            
        return []

    
    
    async def _execute_tool_with_retry(self, tool: str, params: Dict, description: str = "") -> Any:
        """
        LLMãƒ™ãƒ¼ã‚¹ã®è³¢ã„ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ»åˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ 
        
        Args:
            tool: ãƒ„ãƒ¼ãƒ«å
            params: å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            description: ã‚¿ã‚¹ã‚¯ã®èª¬æ˜ï¼ˆLLMåˆ¤æ–­æ™‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        """
        self.logger.info(f"[DEBUG] _execute_tool_with_retry ãŒå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸ: tool={tool}")
        max_retries = 3
        
        # å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒï¼ˆç ´å£Šçš„å¤‰æ›´ã‚’é¿ã‘ã‚‹ï¼‰
        original_params = params.copy()
        current_params = params.copy()
        
        for attempt in range(max_retries + 1):
            # 1. ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œï¼ˆä¾‹å¤–ã‚‚ã‚­ãƒ£ãƒƒãƒã—ã¦çµæœã¨ã—ã¦æ‰±ã†ï¼‰
            try:
                raw_result = await self.connection_manager.call_tool(tool, current_params)
                self.logger.info(f"[DEBUG] ãƒ„ãƒ¼ãƒ«å®Ÿè¡ŒæˆåŠŸ attempt={attempt + 1}, result_type={type(raw_result)}")
            except Exception as e:
                # ä¾‹å¤–ã‚‚ã€Œçµæœã€ã¨ã—ã¦æ‰±ã„ã€LLMåˆ¤æ–­ã«å›ã™
                raw_result = f"ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}"
                self.logger.info(f"[DEBUG] ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ attempt={attempt + 1}, error={type(e).__name__}")
            
            # 2. LLMã«çµæœã‚’åˆ¤æ–­ã•ã›ã‚‹ï¼ˆæˆåŠŸãƒ»å¤±æ•—å•ã‚ãšï¼‰
            try:
                self.logger.info(f"[DEBUG] LLMåˆ¤æ–­ã‚’é–‹å§‹...")
                judgment = await self._judge_and_process_result(
                    tool=tool,
                    current_params=current_params,
                    original_params=original_params,
                    result=raw_result,
                    attempt=attempt + 1,
                    max_retries=max_retries,
                    description=description
                )
                self.logger.info(f"[DEBUG] LLMåˆ¤æ–­å®Œäº†")
                
            except Exception as e:
                self.logger.error(f"[DEBUG] LLMåˆ¤æ–­ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")
                # LLMåˆ¤æ–­ã§ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã€çµæœã‚’ãã®ã¾ã¾è¿”ã™
                return raw_result
            
            # 3. LLMã®åˆ¤æ–­ã«åŸºã¥ã„ã¦è¡Œå‹•
            if judgment.get("needs_retry", False) and attempt < max_retries:
                self.logger.info(f"[ãƒªãƒˆãƒ©ã‚¤] {attempt + 1}/{max_retries}: {judgment.get('error_reason', 'LLMåˆ¤æ–­ã«ã‚ˆã‚‹ãƒªãƒˆãƒ©ã‚¤')}")
                
                # ä¿®æ­£ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å®Ÿè¡Œï¼ˆå…ƒã®paramsã¯ä¿æŒï¼‰
                corrected_params = judgment.get("corrected_params", current_params)
                if corrected_params != current_params:
                    self.logger.info(f"[ä¿®æ­£] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿®æ­£: {safe_str(corrected_params)}")
                    current_params = corrected_params
                
                continue
            
            # æˆåŠŸã¾ãŸã¯æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°åˆ°é”
            return judgment.get("processed_result", raw_result)
        
        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«åˆ°é”
        return judgment.get("processed_result", "æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«åˆ°é”ã—ã¾ã—ãŸã€‚")
    
    async def _judge_and_process_result(
        self, 
        tool: str, 
        current_params: Dict,
        original_params: Dict, 
        result: Any,
        attempt: int = 1,
        max_retries: int = 3,
        description: str = ""
    ) -> Dict[str, Any]:
        """
        LLMã«ã‚ˆã‚‹ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã®åˆ¤æ–­ã¨å‡¦ç†ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
        
        Args:
            tool: ãƒ„ãƒ¼ãƒ«å
            current_params: ç¾åœ¨å®Ÿè¡Œã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            original_params: å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¿®æ­£ã®åŸºæº–ï¼‰
            result: ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ
            attempt: ç¾åœ¨ã®è©¦è¡Œå›æ•°
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            description: ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã®èª¬æ˜
            
        Returns:
            åˆ¤æ–­çµæœè¾æ›¸
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = self._build_judgment_prompt(
            tool=tool,
            current_params=current_params,
            original_params=original_params,
            result=result,
            attempt=attempt,
            max_retries=max_retries,
            description=description
        )
        
        # LLMå‘¼ã³å‡ºã—
        judgment = await self._call_llm_for_judgment(prompt)
        
        # ãƒ­ã‚°å‡ºåŠ›
        self._log_judgment_result(judgment)
        
        return judgment
    
    
    async def _interpret_planned_results(self, user_query: str, results: List[Dict]) -> str:
        """è¨ˆç”»å®Ÿè¡Œã®çµæœã‚’è§£é‡ˆ"""
        # ç¾åœ¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ã€å‰ã®ã‚¿ã‚¹ã‚¯çµæœã®æ··å…¥ã‚’é˜²ã
        recent_context = self._get_conversation_context_only()
        
        # çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºï¼ˆV6å¯¾å¿œï¼‰
        serializable_results = []
        for r in results:
            result_data = {
                "step": r.get("step", r.get("task_description", "ã‚¿ã‚¹ã‚¯")),
                "tool": r.get("tool", "ä¸æ˜"),
                "success": r["success"],
                "description": r.get("description", r.get("task_description", "å®Ÿè¡Œå®Œäº†"))
            }
            
            if r["success"]:
                # æˆåŠŸæ™‚ã¯çµæœã‚’å«ã‚ã‚‹
                max_length = self.config.get("result_display", {}).get("max_result_length", 1000)
                result_str = str(r["result"])
                
                if len(result_str) <= max_length:
                    result_data["result"] = result_str
                else:
                    # é•·ã™ãã‚‹å ´åˆã¯çœç•¥æƒ…å ±ã‚’è¿½åŠ 
                    result_data["result"] = result_str[:max_length]
                    if self.config.get("result_display", {}).get("show_truncated_info", True):
                        result_data["result"] += f"\n[æ³¨è¨˜: çµæœãŒé•·ã„ãŸã‚{max_length}æ–‡å­—ã§çœç•¥ã€‚å®Ÿéš›ã®çµæœã¯ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™]"
            else:
                result_data["error"] = r["error"]
            
            serializable_results.append(result_data)
        
        # ãƒ‡ãƒãƒƒã‚°: LLMã«æ¸¡ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
        if self.verbose:
            self.logger.debug("Serializable results being sent to LLM:")
            for i, result in enumerate(serializable_results):
                result_preview = str(result.get("result", "N/A"))[:100] + "..." if len(str(result.get("result", "N/A"))) > 100 else str(result.get("result", "N/A"))
                self.logger.debug(f"  [{i+1}] Tool: {result['tool']}, Result: {result_preview}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å–å¾—
        prompt = PromptTemplates.get_result_interpretation_prompt(
            recent_context=recent_context,
            user_query=user_query,
            serializable_results=json.dumps(serializable_results, ensure_ascii=False, indent=2),
            custom_instructions=self.custom_instructions
        )

        try:
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            # æœ€çµ‚å¿œç­”ã‚’å–å¾—
            final_response = response.choices[0].message.content
            
            # Rich UIã®å ´åˆã¯ç¾ã—ãè¡¨ç¤º
            if self.ui_mode == "rich" and hasattr(self.display, 'show_result_panel'):
                # JSONã¾ãŸã¯é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‹ã©ã†ã‹åˆ¤å®š
                if len(final_response) > 100 or final_response.strip().startswith('{'):
                    self.display.show_result_panel("å®Ÿè¡Œçµæœ", final_response, success=True)
                
            # å®Ÿè¡Œçµæœã¨å…±ã«å±¥æ­´ã«ä¿å­˜
            self._add_to_history("assistant", final_response, serializable_results)
            
            return final_response
            
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            successful_results = [r for r in results if r["success"]]
            if successful_results:
                return f"å®Ÿè¡Œå®Œäº†ã—ã¾ã—ãŸã€‚{len(successful_results)}å€‹ã®ã‚¿ã‚¹ã‚¯ãŒæˆåŠŸã—ã¾ã—ãŸã€‚"
            else:
                return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _get_recent_context(self, max_items: int = None) -> str:
        """æœ€è¿‘ã®ä¼šè©±æ–‡è„ˆã‚’å–å¾—ï¼ˆV6ç‰ˆ: çŠ¶æ…‹ç®¡ç†ã‹ã‚‰ï¼‰"""
        if max_items is None:
            max_items = self.config["conversation"]["context_limit"]
        
        # çŠ¶æ…‹ç®¡ç†ã‹ã‚‰ä¼šè©±å±¥æ­´ã‚’å–å¾—
        conversation_context = self.state_manager.get_conversation_context(max_items)
        
        if not conversation_context:
            return ""
        
        lines = []
        for entry in conversation_context:
            role = "User" if entry['role'] == "user" else "Assistant"
            # é•·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çœç•¥
            msg = entry['content'][:150] + "..." if len(entry['content']) > 150 else entry['content']
            timestamp = entry.get('timestamp', '')
            if timestamp:
                time_str = timestamp.split('T')[1][:5] if 'T' in timestamp else timestamp
                lines.append(f"[{time_str}] {role}: {msg}")
            else:
                lines.append(f"{role}: {msg}")
        
        return "\n".join(lines)
    
    def _get_conversation_context_only(self, max_items: int = 3) -> str:
        """
        ä¼šè©±æ–‡è„ˆã®ã¿ã‚’å–å¾—ï¼ˆå®Ÿè¡Œçµæœã‚’é™¤å¤–ï¼‰
        çµæœè§£é‡ˆæ™‚ã«å‰ã®ã‚¿ã‚¹ã‚¯çµæœã®æ··å…¥ã‚’é˜²ã
        """
        # V6ã§ã¯çŠ¶æ…‹ç®¡ç†ã‹ã‚‰ä¼šè©±å±¥æ­´ã‚’å–å¾—
        conversation_context = self.state_manager.get_conversation_context(max_items)
        
        if not conversation_context:
            return ""
        
        lines = []
        for entry in conversation_context:
            role = "User" if entry['role'] == "user" else "Assistant"
            # é•·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çœç•¥
            msg = entry['content'][:150] + "..." if len(entry['content']) > 150 else entry['content']
            lines.append(f"{role}: {msg}")
            # å®Ÿè¡Œçµæœã¯å«ã‚ãªã„ï¼ˆæ··å…¥ã‚’é˜²ããŸã‚ï¼‰
        
        return "\n".join(lines)
    
    def _summarize_results(self, results: List[Dict]) -> str:
        """å®Ÿè¡Œçµæœã‚’è¦ç´„ã—ã¦è¡¨ç¤º"""
        summary_parts = []
        for r in results:
            tool = r.get('tool', 'Unknown')
            success = r.get('success', False)
            
            if success and r.get('result'):
                result_str = str(r['result'])
                # çµæœãŒé•·ã„å ´åˆã¯çŸ­ç¸®
                if len(result_str) > 100:
                    result_str = result_str[:97] + "..."
                summary_parts.append(f"{tool}: {result_str}")
            else:
                summary_parts.append(f"{tool}: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        return " | ".join(summary_parts[:3])  # æœ€å¤§3ã¤ã®çµæœã‚’è¡¨ç¤º
    
    def _add_to_history(self, role: str, message: str, execution_results: List[Dict] = None):
        """ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆå®Ÿè¡Œçµæœã‚‚å«ã‚€ï¼‰"""
        history_item = {
            "timestamp": datetime.now().isoformat(),
            "role": role,
            "message": message
        }
        
        # å®Ÿè¡ŒçµæœãŒã‚ã‚Œã°è¿½åŠ 
        if execution_results:
            history_item["execution_results"] = execution_results
        
        self.conversation_history.append(history_item)
        
        # å±¥æ­´ã®é•·ã•åˆ¶é™
        max_history = self.config["conversation"]["max_history"]
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]
    
    def _show_execution_metrics(self):
        """å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º"""
        if not self.config.get("development", {}).get("show_statistics", True):
            return
            
        print("\n" + "=" * 50)
        print("ğŸ“Š å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        print("=" * 50)
        
        # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆçµ±è¨ˆ
        total_attempts = (self.execution_metrics["task_generation_success"] + 
                         self.execution_metrics["task_generation_total_failures"])
        if total_attempts > 0:
            success_rate = (self.execution_metrics["task_generation_success"] / total_attempts) * 100
            print(f"ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”ŸæˆæˆåŠŸç‡: {success_rate:.1f}% ({self.execution_metrics['task_generation_success']}/{total_attempts})")
        
        if self.execution_metrics["task_generation_retry_success"] > 0:
            print(f"ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ: {self.execution_metrics['task_generation_retry_success']}å›")
        
        if self.execution_metrics["json_parse_errors"] > 0:
            print(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {self.execution_metrics['json_parse_errors']}å›")
            
        if self.execution_metrics["timeout_count"] > 0:
            print(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ: {self.execution_metrics['timeout_count']}å›")
            
        if self.execution_metrics["fallback_usage"] > 0:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {self.execution_metrics['fallback_usage']}å›")
        
        if self.execution_metrics["total_task_lists"] > 0:
            avg_tasks = self.execution_metrics["average_task_count"] / self.execution_metrics["total_task_lists"]
            print(f"å¹³å‡ã‚¿ã‚¹ã‚¯æ•°: {avg_tasks:.1f}å€‹")
        
        print("=" * 50)
    
    def _show_session_statistics(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’è¡¨ç¤º"""
        total_time = (datetime.now() - self.session_stats["start_time"]).total_seconds()
        
        self.display.show_result_summary(
            total_tasks=self.session_stats["successful_tasks"] + self.session_stats["failed_tasks"],
            successful=self.session_stats["successful_tasks"],
            failed=self.session_stats["failed_tasks"],
            total_duration=total_time
        )
        
        if self.config["development"]["show_api_calls"]:
            print(f"APIå‘¼ã³å‡ºã—å›æ•°: {self.session_stats['total_api_calls']}")
    
    async def pause_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸€æ™‚åœæ­¢ï¼ˆESCã‚­ãƒ¼å¯¾å¿œï¼‰"""
        await self.state_manager.pause_all_tasks()
        print("\n[ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€æ™‚åœæ­¢] ä½œæ¥­ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        print("æ¬¡å›å†é–‹æ™‚ã«ç¶šãã‹ã‚‰å®Ÿè¡Œã§ãã¾ã™ã€‚")
        return self.state_manager.get_session_summary()
    
    async def resume_session(self) -> Dict[str, Any]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å†é–‹"""
        await self.state_manager.resume_paused_tasks()
        summary = self.state_manager.get_session_summary()
        
        if summary.get("has_work_to_resume", False):
            print(f"\n[ã‚»ãƒƒã‚·ãƒ§ãƒ³å†é–‹] {summary['pending_tasks']}å€‹ã®ã‚¿ã‚¹ã‚¯ãŒå¾…æ©Ÿä¸­ã§ã™")
            
            # å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã¯ç¶™ç¶šå®Ÿè¡Œã‚’ææ¡ˆ
            next_task = self.task_manager.get_next_executable_task()
            if next_task:
                print(f"æ¬¡ã®ã‚¿ã‚¹ã‚¯: {next_task.description}")
        else:
            print("\n[ã‚»ãƒƒã‚·ãƒ§ãƒ³å†é–‹] æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®æº–å‚™å®Œäº†")
        
        return summary
    
    async def clear_session(self):
        """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢"""
        await self.state_manager.clear_current_session()
        print("\n[ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªã‚¢] æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§é–‹å§‹ã—ã¾ã™")
    
    def get_session_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å–å¾—"""
        session_summary = self.state_manager.get_session_summary()
        task_summary = self.task_manager.get_task_summary()
        
        return {
            "session": session_summary,
            "tasks": task_summary,
            "can_resume": session_summary.get("has_work_to_resume", False),
            "ui_mode": self.ui_mode,
            "verbose": self.verbose
        }
    
    async def handle_user_response_to_clarification(self, response: str) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®CLARIFICATIONå›ç­”ã‚’å‡¦ç†
        æ³¨æ„: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯éæ¨å¥¨ã§ã™ã€‚_handle_pending_tasksã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        """
        # æ–°ã—ã„å‡¦ç†ãƒ•ãƒ­ãƒ¼ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
        return await self._handle_pending_tasks(response)
    
    async def _generate_simple_task_list_v6(self, user_query: str) -> List[Dict[str, Any]]:
        """V6ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ"""
        try:
            recent_context = self._get_recent_context()
            tools_info = self.connection_manager.format_tools_for_llm()
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
            prompt = PromptTemplates.get_simple_task_list_prompt(
                recent_context=recent_context,
                user_query=user_query,
                tools_info=tools_info
            )
            
            response = await self.llm.chat.completions.create(
                model=self.config["llm"]["model"],
                messages=[{"role": "system", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            content = safe_str(response.choices[0].message.content)
            result = json.loads(content)
            
            return result.get("tasks", [])
            
        except Exception as e:
            self.logger.error(f"ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆå¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã¯å‰Šé™¤ - ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
            return []
    
    async def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾"""
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        if self.state_manager.current_session:
            await self.state_manager.archive_session()
        
        # çµ‚äº†æ™‚ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        self._show_execution_metrics()
        await self.connection_manager.close()


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    agent = MCPAgent()
    await agent.initialize()
    
    try:
        print("\nMCP Agent ãŒæº–å‚™å®Œäº†ã—ã¾ã—ãŸï¼")
        print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("-" * 60)
        
        while True:
            try:
                if hasattr(agent.display, 'input_prompt') and agent.ui_mode == "rich":
                    user_input = agent.display.input_prompt("Agent").strip()
                else:
                    user_input = input("\nAgent> ").strip()
            except (EOFError, KeyboardInterrupt):
                break
            
            if user_input.lower() in ['quit', 'exit', 'çµ‚äº†']:
                break
            
            if not user_input:
                continue
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
            response = await agent.process_request(user_input)
            
            # Rich UIã®å ´åˆã¯Markdownæ•´å½¢è¡¨ç¤º
            if agent.ui_mode == "rich" and hasattr(agent.display, 'show_markdown_result'):
                agent.display.show_markdown_result(response)
            else:
                print(f"\n{response}")
    
    except KeyboardInterrupt:
        print("\n\n[ä¸­æ–­] Ctrl+CãŒæŠ¼ã•ã‚Œã¾ã—ãŸã€‚")
    finally:
        await agent.close()
        print("\nMCP Agent ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    asyncio.run(main())